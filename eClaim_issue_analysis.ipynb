{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzYRa7dx0O7TFJ9wHsCJ8w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linufx2208-sketch/JHU_Learnings/blob/main/eClaim_issue_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MutOvwta3ESe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b601685f"
      },
      "source": [
        "# Task\n",
        "Upload the CSV file containing customer issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f78f2b51"
      },
      "source": [
        "## Upload and Load Data\n",
        "\n",
        "### Subtask:\n",
        "Instruct the user to upload the CSV file containing the issues. Once uploaded, load the data into a pandas DataFrame for processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ab3a5d6"
      },
      "source": [
        "Please upload the `customer_issues.csv` file. You can do this by clicking the folder icon on the left panel, then clicking the 'Upload to session storage' icon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "487674c8"
      },
      "source": [
        "**Reasoning**:\n",
        "After instructing the user to upload the file, the next step is to provide the Python code to load the `customer_issues.csv` file into a pandas DataFrame, as specified in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6840508",
        "outputId": "ec5b629c-3399-4665-dbc6-b6bc522538d5"
      },
      "source": [
        "import pandas as pd # Import the pandas library, commonly aliased as 'pd', for data manipulation\n",
        "\n",
        "try: # Start a try block to gracefully handle potential errors during file loading\n",
        "    df = pd.read_csv('eClaim_Customer_issues.csv') # Attempt to read the CSV file into a pandas DataFrame named 'df'\n",
        "    print(\"eClaim_Customer_issues loaded successfully!\") # Print a success message if the file is loaded\n",
        "    print(df.head()) # Display the first 5 rows of the DataFrame to verify content\n",
        "except FileNotFoundError: # Catch the specific error if the file is not found\n",
        "    print(\"Error: 'eClaim_Customer_issues' not found. Please ensure the file is uploaded correctly.\") # Inform the user about the missing file\n",
        "except Exception as e: # Catch any other unexpected errors that might occur\n",
        "    print(f\"An error occurred while loading the file: {e}\") # Print a generic error message with the exception details"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eClaim_Customer_issues loaded successfully!\n",
            "   Sl.No. Reported Date Reported By  \\\n",
            "0     1.0     15-Jul-25       Silpa   \n",
            "1     2.0     15-Jul-25       Silpa   \n",
            "2     3.0     17-Jul-25       Silpa   \n",
            "3     4.0     17-Jul-25       Silpa   \n",
            "4     5.0     18-Jul-25       Silpa   \n",
            "\n",
            "                                              Issues     Mode         Type  \\\n",
            "0               Object reference error while AI scan  Backend  Phase 1-Bug   \n",
            "1  For Barcode scanning, the response shows as  '...  Backend  Phase 1-Bug   \n",
            "2  DOT format is 13 and 11 digits not 13 and 12 d...  Backend      Phase 1   \n",
            "3  Need to amend the duplicate image message pop ...  Backend      Phase 1   \n",
            "4  Tire data is not getting resgitered while scan...      App      Phase 1   \n",
            "\n",
            "   Status BS  \n",
            "0  Completed  \n",
            "1  Completed  \n",
            "2  Completed  \n",
            "3  Completed  \n",
            "4        NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ff42eae"
      },
      "source": [
        "## Perform Sentiment Analysis\n",
        "\n",
        "### Subtask:\n",
        "Apply an AI model or library to perform sentiment analysis on the 'issues' column in the DataFrame. This step will categorize each issue's sentiment (e.g., positive, negative, neutral) and add it as a new column to the DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2f7bbb7",
        "outputId": "faaba595-bc5e-4186-cc17-5c11021d5064"
      },
      "source": [
        "import nltk # Import the Natural Language Toolkit library\n",
        "\n",
        "print(\"Installing textblob and downloading NLTK corpora...\") # Inform the user about the upcoming installation and downloads\n",
        "\n",
        "# Install textblob if not already installed\n",
        "!pip install -qq textblob # Use pip to quietly install the textblob library\n",
        "\n",
        "# Download necessary NLTK corpora\n",
        "try: # Start a try block to check for 'wordnet' corpus\n",
        "    nltk.data.find('corpora/wordnet.zip') # Check if 'wordnet' data is already present locally\n",
        "except LookupError: # If 'wordnet' is not found (LookupError occurs)\n",
        "    nltk.download('wordnet', quiet=True) # Download 'wordnet' corpus quietly\n",
        "\n",
        "try: # Start a try block to check for 'punkt' corpus\n",
        "    nltk.data.find('corpora/punkt.zip') # Check if 'punkt' data is already present locally\n",
        "except LookupError: # If 'punkt' is not found (LookupError occurs)\n",
        "    nltk.download('punkt', quiet=True) # Download 'punkt' corpus quietly\n",
        "\n",
        "print(\"TextBlob and NLTK corpora setup complete.\") # Print a message indicating completion of setup"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing textblob and downloading NLTK corpora...\n",
            "TextBlob and NLTK corpora setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "79fae133",
        "outputId": "8fa6aaee-4dad-4a11-9355-23e55cb7175e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style for better aesthetics\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Create a bar plot of sentiment distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Sentiment', data=df, palette='viridis', order=df['Sentiment'].value_counts().index)\n",
        "plt.title('Distribution of Customer Issue Sentiments')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Number of Issues')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1397762802.py:9: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.countplot(x='Sentiment', data=df, palette='viridis', order=df['Sentiment'].value_counts().index)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq4AAAIjCAYAAADC0ZkAAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAARk9JREFUeJzt3XlcVGX///E3ICgooiFuuGuMqCiQZoBLUrmmmWtmmGWmmdpmt6j3nUsmZJqlpuFSGlbanUpZLqVpiqFpWe7cJe4rYokGCgzz+6Mv82sElMHB8dTr+Xj0eDTXOXNdn5k5h3l75jrnuFgsFosAAACA25yrswsAAAAAioLgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCthp1qxZMplMt2SsqKgoRUVFWR9v375dJpNJa9euvSXjR0dHKzIy8paMVVx//PGHxo0bp4iICJlMJr322mvOLglwiGv3fwBSKWcXADjTihUrNGbMGOtjDw8P+fj4yGQyqW3bturRo4fKlSt30+OcPXtWn3zyie6//34FBgbedH+OdDvXVhRxcXFauXKlhg0bppo1a6p+/frXXd9sNishIUEJCQlKTk5WRkaGKleurJYtW+rRRx9VUFCQw2s0+nt8I5GRkbrzzjsVFxfn7FKKbefOnXr33XeVnJys33//Xb6+vmrYsKG6dOmirl27lti4v/76q9asWaOHH35YNWrUKLFxSsrffdvG7YfgCkgaOXKkatSooZycHJ0/f17ff/+9pkyZokWLFmnOnDlq2LChdd1nnnlGTz/9tF39nzt3TrNnz5a/v79df9wXLlxo1zjFcb3aXn31VVkslhKv4WZs27ZNzZo10/Dhw2+47pUrVzR8+HBt2bJFLVq00JAhQ+Tj46OTJ09qzZo1WrlypTZt2qSqVas6tMbifv64NdasWaMXXnhBgYGBGjBggHx8fHTixAnt2LFDn3zySYkH19mzZ+vuu+/OF1xvxf5/s9i2casRXAFJbdq0sTnSNmTIECUlJWno0KEaNmyYVq9erTJlykiSSpUqpVKlSnbXyczMlKenpzw8PEp0nBtxd3d36vhFkZaWpgYNGhRp3alTp2rLli0aM2aMBg4caLNs+PDhWrRokeML/BvIyclRbm6u07fHkjJ79mw1aNBAy5Yty/ca09LSnFSV/rbvN3AzmOMKFCIsLEzDhg3TyZMn9fnnn1vbC5rjunXrVvXr10/NmzdXSEiIOnTooDfffFPSn/NSe/XqJUkaM2aMTCaTTCaTVqxYIenPeWwPPvig9u7dq/79+6tZs2bW5xY2xy03N1dvvvmmIiIiFBwcrKFDh+r06dM260RGRio6Ojrfc//a541qK2iOa0ZGhmJjY9W2bVs1adJEHTp00MKFC/MdmTWZTJo0aZLWr1+vBx98UE2aNFGXLl20efPm673tVmlpaRo7dqzCw8MVFBSkbt26aeXKldblefN9T5w4oU2bNllrP3HiRIH9nTlzRsuWLVNERES+0CpJbm5uGjRokPVoa2Hzex39+Ut/HvHr0aOHmjZtqpYtW2rUqFE6e/aszRjR0dEKCQnRqVOnNGTIEIWEhKh169b68MMPJUnJyckaMGCAgoOD1a5dO61atSpf7enp6Xrttdesn90DDzygefPmKTc317rOiRMnZDKZtHDhQi1atEj333+/goKCdOjQoQLf18J8+eWX6tGjh0JCQhQaGqquXbtq8eLF1uXZ2dmaPXu22rdvr6CgILVs2VL9+vXT1q1bresUtv0X9Nnk5uZq0aJF6tKli4KCghQeHq5XXnlFFy9evGGtx44dU1BQUIFB0dfXt1jjREZGasiQIdq5c6d69eqloKAg3XfffUpISLCus2LFCj333HOSpAEDBli3je3btxf4+vO2+dWrV2v27Nlq3bq1QkJCNHLkSF26dElZWVl67bXXFBYWppCQEI0ZM0ZZWVn5XtNnn31m3d7uvvtuvfDCC/n+fuT9Xfr1118VFRWlZs2aqXXr1po/f75NPdfbto8cOaIRI0YoIiJCQUFBatOmjV544QVdunTphp8JUBiOuALX8dBDD+nNN99UYmKi+vTpU+A6v/zyi4YMGSKTyaSRI0fKw8NDR48e1Y8//ihJql+/vkaOHKmZM2eqb9++uuuuuyRJoaGh1j5+//13DR48WF26dFG3bt3yfVlea+7cuXJxcdHgwYOVlpamxYsXa+DAgfrss8+sR4aLoii1/ZXFYtEzzzxj/cIKDAzUli1bNHXqVJ09e1Zjx461Wf+HH37QV199pUcffVRly5ZVfHy8Ro4cqY0bN6pixYqF1nXlyhVFRUXp2LFj6t+/v2rUqKG1a9cqOjpa6enpevzxx1W/fn1NnTpVMTExqlq1qp544glJ0h133FFgn5s3b1ZOTo66detW5PenKG7288+bZx0UFKQXX3xRaWlp+uCDD/Tjjz8qISFB5cuXt45lNps1ePBgNW/eXKNGjdKqVas0adIkeXp6asaMGeratavat2+vpUuXavTo0QoODlbNmjUl/XkU/7HHHtPZs2f1yCOPqFq1atq1a5fefPNNpaamaty4cTava8WKFbp69ar69OljnftdVFu3btWLL76osLAwjRo1SpKUkpKiH3/8UY8//rikP49yxsXFqXfv3mratKkuX76svXv3at++fYqIiLD7c3jllVe0cuVK9ejRQ1FRUTpx4oQ+/PBD7d+/Xx9//PF1fz2oXr26kpKSdObMmRtOE7FnnKNHj+q5555Tr1699PDDD2v58uWKjo5W48aNdeedd6pFixaKiopSfHy8hg4dqnr16knSDedpz5s3T2XKlNHTTz+to0ePasmSJSpVqpRcXFyUnp6u4cOH6+eff9aKFSvk7+9vM41m7ty5evvtt9WpUyf16tVLFy5c0JIlS9S/f/9829vFixf11FNP6YEHHlCnTp20bt06TZs2TQEBAWrbtu11t+2srCwNGjRIWVlZeuyxx1SpUiWdPXtWmzZtUnp6ury9va/7GoFCWYB/sOXLl1sCAgIsu3fvLnSdu+66y9K9e3fr45kzZ1oCAgKsj99//31LQECAJS0trdA+du/ebQkICLAsX74837LHHnvMEhAQYPn4448LXPbYY49ZH2/bts0SEBBgad26teXSpUvW9tWrV1sCAgIsixcvtra1a9fOMnr06Bv2eb3aRo8ebWnXrp318ddff20JCAiwzJkzx2a9ESNGWEwmk+Xo0aPWtoCAAEvjxo1t2g4cOGAJCAiwxMfH5xvrrxYtWmQJCAiwfPbZZ9a2rKwsS9++fS3BwcE2r71du3aWp59++rr9WSwWy5QpUywBAQGW/fv333BdiyX/a8/jyM8/KyvLEhYWZnnwwQctV65csbZv3LjREhAQYHn77bdt6gkICLC8++671raLFy9amjZtajGZTJYvv/zS2n7o0CFLQECAZebMmda2d955xxIcHGw5fPiwTQ3Tpk2zBAYGWk6dOmWxWCyW48ePWwICAiyhoaHXfU1/de1nMHnyZEtoaKglJyen0Od069bthp/btdtqnms/mx07dlgCAgIsn3/+uc16mzdvLrD9Wv/973+t22tUVJTlrbfesuzYscNiNptt1rNnnHbt2lkCAgIsO3bssLalpaVZmjRpYomNjbW2rVmzxhIQEGDZtm3bDV9/3v7/4IMPWrKysqztL774osVkMlmeeuopm+f37dvX5n06ceKEJTAw0DJ37lyb9ZKTky2NGjWyac/7u7Ry5Upr29WrVy0RERGWESNGWNsK27b3799vCQgIsKxZsybf6wJuBlMFgBvw8vLSH3/8UejyvCMUGzZssPnJ1R4eHh7q0aNHkdfv3r27zdUOOnbsKD8/P3377bfFGr+oNm/eLDc3t3w/3z755JOyWCz5pgGEh4erVq1a1scNGzZUuXLldPz48RuO4+fnpwcffNDa5u7urqioKGVkZGjHjh1213758mVJUtmyZe1+7vXczOe/d+9epaWlqV+/fipdurS1/d5771W9evW0adOmfM/p3bu3zdh169aVp6enOnXqZG2vV6+eypcvb/M+r127VnfddZfKly+vCxcuWP8LDw+X2WzO9562b9++0KPXN1K+fHllZmba/Oxf0Dq//PKLjhw5Uqwx/mrt2rXy9vZWRESEzWtr3LixvLy8rD+9F6ZXr15asGCBWrZsqR9//FFz5sxR//791b59e+uR8+KM06BBAzVv3tz6+I477lDdunVvuP3fyEMPPWRzZLdp06ayWCzq2bOnzXpNmzbV6dOnlZOTI0n6+uuvlZubq06dOtnUX6lSJdWuXTtf/V5eXnrooYesjz08PBQUFFSk+vP+PiUmJiozM7PYrxW4FlMFgBvIyMi47k/3nTt31n//+1/9+9//1vTp0xUWFqYHHnhAHTt2lKtr0f5tWKVKFbtOxKhdu7bNYxcXF9WuXVsnT54sch/FcfLkSVWuXDnfJcLyftq8dvxq1arl68PHx0fp6ek3HKd27dr53r+8cU6dOmV37Xk1X+8fIcVxM59/3uuoW7duvmX16tXTDz/8YNNWunTpfGHS29tbVatWlYuLS772v77PR48eVXJyssLCwgqs5cKFCzaPb+bSTI8++qjWrFmjwYMHq0qVKoqIiFCnTp3Upk0b6zojR47UsGHD1KFDBwUEBKhVq1Z66KGHbK7gUVRHjx7VpUuXCn1tRTnBqnXr1mrdurUyMzO1b98+rV69WkuXLtXQoUO1Zs0a+fr62j1OYdt/UebdXk/16tVtHuf97H7teN7e3srNzdWlS5dUsWJFHTlyRBaLRe3bty+w32tPOi1ou/Lx8VFycvINa6xZs6aeeOIJvf/++1q1apWaN2+uyMhIdevWjWkCuCkEV+A6zpw5o0uXLtkcNbxWmTJl9OGHH2r79u3atGmTtmzZotWrV2vZsmV677335ObmdsNx7JmXerPMZnORanKEwsaxOOESW3nzB5OTk4t02Z5rv7DzmM1mm8eO+PyLqrC+ivI+5+bmKiIiQk899VSB69apU8fm8c1sk76+vkpISFBiYqI2b96szZs3a8WKFerevbtef/11SVKLFi309ddfa8OGDdq6das+/fRTLV68WBMnTrQ5qlyQaz+D3Nxc+fr6atq0aQWub8+RY09PTzVv3lzNmzdXxYoVNXv2bG3evFkPP/yw3eOU1H5W2D+ICmvP2w5yc3Pl4uKi+fPnF1ibl5eXzeObrT86OloPP/yw9TOePHmy4uLi9Mknnzj8knP45yC4Atfx2WefSZJatWp13fVcXV0VFhamsLAwjRkzRu+++65mzJih7du3Kzw8vNAQVFxHjx61eWyxWHT06FGbs90LO7J56tQp6wk7UuEBrSD+/v5KSkrS5cuXbY66pqSkWJc7gr+/v5KTk5Wbm2vzZZw3zrVHnIqiTZs2cnNz06pVq9S9e/cbrl++fPlC379rFffzz3sdhw8fzncU7/Dhw8V6nYWpVauWMjIyFB4e7rA+r8fDw0ORkZGKjIxUbm6uJkyYoGXLlmnYsGHWXwwqVKignj17qmfPnvrjjz/02GOPadasWdbg6uPjU+DP0td+BrVq1VJSUpJCQ0Md+o/AJk2aSJJSU1NLbBxH/224nlq1aslisahGjRoFHuUvjhvVn3elgWHDhunHH39Uv3799PHHH+uFF15wyPj452GOK1CIpKQkzZkzRzVq1Ljumei///57vra8I3p5l6Lx9PSUpBv+RF5UCQkJ1jmb0p9z71JTU21+iq1Zs6Z+/vlnm8vhbNy4Md9lb+yprU2bNjKbzdZLMOVZtGiRXFxcbMa/GW3atFFqaqpWr15tbcvJyVF8fLy8vLzUokULu/usVq2aevfurcTERMXHx+dbnpubq/fee09nzpyR9OeX/KVLl3Tw4EHrOufOndPXX39t87yb+fybNGkiX19fLV261OZz+vbbb3Xo0CHde++9dr/OwnTq1Em7du3Sli1b8i1LT0+3zoN0hN9++83msaurq/UfVXmv89p1ypYtq1q1atm8DzVr1lRKSorNNIaDBw/azDuV/nxtZrNZc+bMyVdLTk7ODbftpKSkAtvz5oznhbybHacgedvGrbhEVPv27eXm5qbZs2fn+9XDYrHk+0yKorBt+/Lly/m2qYCAALm6uhZ4iS6gqDjiCujPk4FSUlJkNpt1/vx5bd++XVu3blX16tU1d+5cmxNnrvXOO+9o586datu2rfz9/ZWWlqaPPvpIVatWtV4eplatWipfvryWLl2qsmXLysvLS02bNrU58mkPHx8fPfroo+rRo4f1cli1a9e2uWRX7969tW7dOj311FPq1KmTjh07plWrVuWb9mBPbZGRkWrZsqVmzJihkydPymQyaevWrdqwYYMef/zx606psEffvn21bNkyRUdHa9++ffL399e6dev0448/auzYscW+DW90dLSOHz+uyZMn66uvvlK7du1Uvnx5nT59WmvXrlVKSoq6dOki6c+5q9OmTdPw4cMVFRWlK1eu6OOPP1bdunW1b98+a583+/mPGjVKY8aM0WOPPaYuXbpYL4fl7+9f4PVmi2vQoEH65ptvNHToUD388MNq3LixMjMz9b///U/r1q3Thg0bin0y1rX+/e9/6+LFi7rnnntUpUoVnTp1SkuWLFFgYKB1nnKXLl109913q3HjxqpQoYL27NmjdevW6bHHHrP206tXLy1atEiDBg1Sr169lJaWpqVLl6pBgwY2c5Xvvvtu9e3bV3FxcTpw4IAiIiLk7u6uI0eOaO3atRo3bpw6duxYaL3Dhg1TjRo11K5dO9WsWVOZmZn67rvvtHHjRgUFBaldu3YOGacggYGBcnNz0/z583Xp0iV5eHjonnvuueEl8YqjVq1aev755zV9+nSdPHlS999/v8qWLasTJ05o/fr16tOnjwYNGmR3nwVt28nJyZo0aZI6duyoOnXqyGw267PPPpObm5s6dOjg8NeGfw6CKyBp5syZkv48c71ChQoKCAjQ2LFj1aNHjxuGpMjISJ08eVLLly/Xb7/9pooVK+ruu+/WiBEjrCchuLu7KzY2Vm+++aYmTJignJwcxcTEFDu4Dh06VMnJyZo3b57++OMPhYWFafz48dajH9KfJ5tER0fr/fff15QpU9SkSRO9++671jmGeeypzdXVVXPnztXMmTO1evVq63Ui//Wvf+nJJ58s1mspSJkyZRQfH69p06Zp5cqVunz5surWrauYmBi7rr5wLU9PT82fP18rVqxQQkKC5syZoytXrqhy5cpq2bKlpk2bpipVqkiSdX5jbGys3njjDdWoUUMvvviijh49ahNcb/bz79Gjh8qUKaP58+dr2rRp8vLy0v3336+XX37Z5pqaN8vT01Px8fGKi4vT2rVrlZCQoHLlyqlOnTo2tTpCt27d9Mknn+ijjz5Senq6/Pz81KlTJ40YMcI69SMqKkrffPONtm7dqqysLFWvXl3PP/+8TXCqX7++Xn/9dc2cOVMxMTFq0KCBpk6dqi+++ELff/+9zZiTJk1SkyZNtHTpUs2YMUNubm7y9/dXt27dCr0ucZ7Jkydrw4YNWrNmjc6dOyeLxaKaNWtq6NChGjx4sM1JSzczTkH8/Pw0ceJExcXFady4cTKbzfrggw9KJLhK0tNPP606depo0aJFeueddyT9eRJWREREgTfcuJHCtu0WLVqoVatW2rhxo86ePStPT0+ZTCbNnz9fwcHBDn5V+CdxsTjjLAkAAADATsxxBQAAgCEQXAEAAGAIBFcAAAAYAsEVAAAAhkBwBQAAgCEQXAEAAGAIf/vruObm5ionJ0eurq639NZ6AAAAKBqLxaLc3FyVKlXK5lbf1/rbB9ecnBzt2bPH2WUAAADgBoKCguTh4VHo8r99cM1L7UFBQXJzc3NyNQAAALiW2WzWnj17rnu0VfoHBNe86QFubm4EVwAAgNvYjaZ1cnIWAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK7FZM7NdXYJgA22SQDA310pZxdgVG6uroqdtULHTp53dimAavlXUvSIHs4uAwCAEkVwvQnHTp7Xr4fPOLsMAACAfwSmCgAAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwhNsmuM6bN08mk0mvvfaate3q1auaOHGiWrZsqZCQEI0YMULnz593YpUAAABwltsiuO7evVtLly6VyWSyaZ8yZYo2btyot956S/Hx8Tp37pyGDx/upCoBAADgTE4Prn/88YdefvllTZ48WT4+Ptb2S5cuafny5YqOjlZYWJiaNGmiKVOmaNeuXfrpp5+cVzAAAACcopSzC5g0aZLatm2r8PBwzZ0719q+d+9eZWdnKzw83NpWv359Va9eXT/99JOCg4PtGsdsNjuqZEmSm5ubQ/sDHMHR2zkAALdCUb+/nBpcv/zyS+3fv1+ffvppvmXnz5+Xu7u7ypcvb9Pu6+ur1NRUu8fas2dPseu8lqenpxo1auSw/gBHSU5OVmZmprPLAACgRDgtuJ4+fVqvvfaa3nvvPZUuXbrExwsKCuIoKf72rp0nDgCAEZjN5iIdZHRacN23b5/S0tLUo0cPa5vZbNaOHTv04YcfauHChcrOzlZ6errNUde0tDT5+fnZPZ6bmxvBFX97bOMAgL8zpwXXe+65R6tWrbJpGzNmjOrVq6fBgwerWrVqcnd3V1JSkjp06CBJSklJ0alTp+ye3woAAADjc1pwLVeunAICAmzavLy8VKFCBWt7z549FRsbKx8fH5UrV06TJ09WSEgIwRUAAOAfyOlXFbiesWPHytXVVSNHjlRWVpZatWql8ePHO7ssAAAAOMFtFVzj4+NtHpcuXVrjx48nrAIAAMD5NyAAAAAAioLgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAENwanD96KOP1LVrV4WGhio0NFR9+/bVt99+a11+9epVTZw4US1btlRISIhGjBih8+fPO7FiAAAAOItTg2vVqlU1atQorVixQsuXL9c999yjZ599Vr/88oskacqUKdq4caPeeustxcfH69y5cxo+fLgzSwYAAICTlHLm4JGRkTaPX3jhBX388cf66aefVLVqVS1fvlzTpk1TWFiYpD+DbOfOnfXTTz8pODjYCRUDAADAWZwaXP/KbDZr7dq1ysjIUEhIiPbu3avs7GyFh4db16lfv76qV69erOBqNpsdWq+bm5tD+wMcwdHbOQAAt0JRv7+cHlyTk5P1yCOP6OrVq/Ly8tI777yjBg0a6MCBA3J3d1f58uVt1vf19VVqaqrd4+zZs8dRJcvT01ONGjVyWH+AoyQnJyszM9PZZQAAUCKcHlzr1q2rhIQEXbp0SevWrdPo0aO1ZMkSh48TFBTEUVL87ZlMJmeXAACA3cxmc5EOMjo9uHp4eKh27dqSpCZNmmjPnj364IMP1KlTJ2VnZys9Pd3mqGtaWpr8/PzsHsfNzY3gir89tnEAwN/ZbXcd19zcXGVlZalJkyZyd3dXUlKSdVlKSopOnTrFiVkAAAD/QE494jp9+nS1adNG1apV0x9//KEvvvhC33//vRYuXChvb2/17NlTsbGx8vHxUbly5TR58mSFhIQQXAEAAP6B7A6uK1euVMWKFXXvvfdKkqZOnapPPvlEDRo00PTp0+Xv71/kvtLS0jR69GidO3dO3t7eMplMWrhwoSIiIiRJY8eOlaurq0aOHKmsrCy1atVK48ePt7dkAAAA/A3YHVzfffddTZgwQZK0a9cuffTRRxozZow2btyomJgYzZ49u8h9TZky5brLS5curfHjxxNWAQAAYH9wPXPmjPVkqvXr16t9+/bq27evQkNDFRUV5fACAQAAAKkYJ2d5eXnp999/lyRt3brVeoOA0qVL6+rVqw4tDgAAAMhj9xHX8PBw/fvf/1ZgYKCOHDmitm3bSpJ++eUXu+a3AgAAAPaw+4jr+PHjFRwcrAsXLmjmzJmqWLGiJGnfvn3q0qWLwwsEAAAApGIccS1fvrxeeeWVfO0jR450SEEAAABAQYp1A4KdO3dq1KhReuSRR3T27FlJUkJCgnbu3OnQ4gAAAIA8dgfXdevWadCgQSpTpoz27dunrKwsSdLly5cVFxfn8AIBAAAAqRjBde7cuZo4caImT56sUqX+/0yD0NBQ7d+/36HFAQAAAHnsDq6HDx9W8+bN87V7e3srPT3dIUUBAAAA17I7uFaqVEnHjh3L1/7DDz+oZs2aDikKAAAAuJbdwbVPnz567bXX9PPPP8vFxUVnz57V559/rtdff139+vUriRoBAAAA+y+H9fTTTys3N1cDBw5UZmamHnvsMXl4eOjJJ5/klq8AAAAoMXYHVxcXFz3zzDMaNGiQjh07poyMDNWvX19ly5YtifoAAAAAScUIrnk8PDzUoEEDR9YCAAAAFMru4BoVFSUXF5dCl3/wwQc3VRAAAABQELuDa2BgoM3jnJwcHThwQL/88ou6d+/uqLoAAAAAG3YH17FjxxbYPmvWLGVkZNx0QQAAAEBB7L4cVmG6deum5cuXO6o7AAAAwIbDguuuXbvk4eHhqO4AAAAAG3ZPFRg+fLjNY4vFotTUVO3du1fDhg1zWGEAAADAX9kdXL29vW0eu7i4qG7duho5cqRatWrlsMIAAACAv7I7uMbExJREHQAAAMB12R1cT58+LRcXF1WtWlWStHv3bq1atUoNGjRQ3759HV4gAAAAIBXj5KyXXnpJ27ZtkySlpqZq4MCB2rNnj2bMmKHZs2c7vEAAAABAKkZw/eWXX9S0aVNJ0po1axQQEKClS5dq2rRpWrlypcMLBAAAAKRiBNecnBzrZa++++47RUZGSpLq1aun1NRUx1YHAAAA/B+7g2uDBg20dOlS7dy5U999953atGkjSTp37pwqVKjg6PoAAAAAScUIrqNGjdKyZcsUFRWlLl26qGHDhpKkb775xjqFAAAAAHA0u68q0LJlS23btk2XL1+Wj4+Ptb1Pnz7y9PR0aHEAAABAHruPuF65ckVZWVnW0Hry5EktWrRIhw8flq+vr8MLBAAAAKRiBNdhw4YpISFBkpSenq4+ffro/fff17PPPquPPvrI0fUBAAAAkooRXPft26fmzZtLktatWydfX19t3LhRr7/+uuLj4x1eIAAAACAVc6pA2bJlJUmJiYlq3769XF1dFRwcrFOnTjm8QAAAAEAqRnCtVauW1q9fr9OnTysxMVERERGSpLS0NJUrV87hBQIAAABSMYLrs88+q6lTpyoyMlLNmjVTSEiIJGnr1q0KDAx0eIEAAACAVIzLYXXs2FF33XWXUlNTrddwlaSwsDDdf//9Di0OAAAAyGN3cJUkPz8/+fn52bRx8wEAAACUpCIH1+HDhxdpvdmzZxe7GAAAAKAwRQ6u3t7eJVkHAAAAcF1FDq4xMTElWQcAAABwXXZfVQAAAABwBoIrAAAADIHgCgAAAEMguAIAAMAQihRcH374YV28eFHSn5e7yszMLNGiAAAAgGsVKbgeOnTIGlbfeecdZWRklGhRAAAAwLWKdDmswMBAjRkzRnfddZcsFosWLlwoLy+vAtct6o0KAAAAAHsUKbjGxMRo1qxZ2rhxo1xcXLRlyxa5ubnlW8/FxYXgCgAAgBJRpOBar149zZgxQ5LUsGFDLVq0SL6+viVaGAAAAPBXRb5zVp6DBw+WRB0AAADAddkdXCXp2LFjWrx4sQ4dOiRJatCggQYMGKBatWo5tDgAAAAgj93Xcd2yZYs6d+6s3bt3y2QyyWQy6eeff1aXLl20devWkqgRAAAAsP+I6/Tp0zVw4ECNGjXKpn3atGmaNm2aIiIiHFYcAAAAkMfuI66HDh1Sr1698rX37NlTv/76q0OKAgAAAK5ld3C94447dODAgXztBw4c4EoDAAAAKDF2TxXo3bu3XnnlFR0/flyhoaGSpB9//FHz58/XwIEDHV0fAAAAIKkYwfXZZ59VuXLl9N577+nNN9+UJFWuXFnDhw/XgAEDHF4gAAAAIBUjuLq4uGjgwIEaOHCgLl++LEkqV66cwwsDAAAA/qpY13HNQ2AFAADArWL3yVkAAACAMxBcAQAAYAgEVwAAABiCXcE1Oztbjz/+uI4cOVJC5QAAAAAFsyu4uru7Kzk5uaRqAQAAAApl91SBbt266dNPPy2JWgAAAIBC2X05LLPZrI8//ljfffedmjRpIk9PT5vlY8aMcVhxAAAAQB67g+v//vc/NWrUSJJ0+PBhm2UuLi6OqQoAAAC4ht3BNT4+viTqAAAAAK6r2JfDOnr0qLZs2aIrV65IkiwWi8OKAgAAAK5l9xHX3377Tc8//7y2b98uFxcXffXVV6pZs6bGjh0rHx8fRUdHl0SdAAAA+Iez+4hrTEyMSpUqpU2bNqlMmTLW9s6dO2vLli0OLQ4AAADIY/cR161bt2rhwoWqWrWqTXudOnV06tQphxUGAAAA/JXdR1wzMjJsjrTm+f333+Xh4eGQogAAAIBr2R1cmzdvroSEBJu23NxcLViwQC1btnRUXQAAAIANu6cKvPzyyxo4cKD27t2r7OxsvfHGG/r111918eJFffzxxyVRIwAAAGB/cA0ICNC6deu0ZMkSlS1bVhkZGXrggQfUv39/Va5cuSRqBAAAAOwPrpLk7e2tZ555xtG1AAAAAIUqVnC9ePGiPv30Ux06dEiS1KBBA/Xo0UMVKlRwZG0AAACAld0nZ+3YsUORkZGKj49Xenq60tPTFR8fr/vuu087duwoiRoBAAAA+4+4Tpo0SZ07d9aECRPk5uYmSTKbzZo4caImTZqkVatWObxIAAAAwO4jrkePHtUTTzxhDa2S5ObmpoEDB+ro0aMOLQ4AAADIY3dwbdSokVJSUvK1p6SkqGHDhg4pCgAAALhWkaYKHDx40Pr/AwYM0GuvvaajR4+qWbNmkqSff/5ZH374oUaNGlUyVQIAAOAfr0jBtXv37nJxcZHFYrG2vfHGG/nWe+mll9S5c2fHVQcAAAD8nyIF1w0bNpR0HQAAAMB1FSm4+vv7l3QdAAAAwHUV6wYEZ8+e1Q8//KALFy4oNzfXZtmAAQMcUhgAAADwV3YH1xUrVuiVV16Ru7u7KlasaLPMxcXFruAaFxenr776SikpKSpTpoxCQkI0atQo1atXz7rO1atXFRsbq9WrVysrK0utWrXS+PHjValSJXtLBwAAgIHZHVzffvttPfvssxoyZIhcXe2+mpaN77//Xv3791dQUJDMZrPefPNNDRo0SF9++aW8vLwkSVOmTNG3336rt956S97e3nr11Vc1fPhwLV269KbGBgAAgLHYHVyvXLmiLl263HRolaSFCxfaPI6NjVVYWJj27dunFi1a6NKlS1q+fLmmTZumsLAwSX8G2c6dO+unn35ScHDwTdcAAAAAY7A7uPbs2VNr167V008/7fBiLl26JEny8fGRJO3du1fZ2dkKDw+3rlO/fn1Vr17d7uBqNpsdWutf7xwG3C4cvZ0DAHArFPX7y+7g+tJLL2nIkCHasmWLAgICVKqUbRdjxoyxt0tJUm5urqZMmaLQ0FAFBARIks6fPy93d3eVL1/eZl1fX1+lpqba1f+ePXuKVVdBPD091ahRI4f1BzhKcnKyMjMznV0GAAAlwu7gGhcXp8TERNWtWzffMhcXl2IXMnHiRP3yyy/66KOPit3H9QQFBXGUFH97JpPJ2SUAAGA3s9lcpIOMdgfX999/X1OmTFGPHj2KVVhBJk2apE2bNmnJkiWqWrWqtb1SpUrKzs5Wenq6zVHXtLQ0+fn52TWGm5sbwRV/e2zjAIC/M7vPsPLw8FBoaKhDBrdYLJo0aZK+/vprLV68WDVr1rRZ3qRJE7m7uyspKcnalpKSolOnTnFiFgAAwD+M3cF1wIABWrJkiUMGnzhxoj7//HNNnz5dZcuWVWpqqlJTU3XlyhVJkre3t3r27KnY2Fht27ZNe/fu1dixYxUSEkJwBQAA+Iexe6rA7t27tW3bNm3cuFF33nlnvpOzZs+eXeS+Pv74Y0lSVFSUTXtMTIx1KsLYsWPl6uqqkSNH2tyAAAAAAP8sdgfX8uXLq3379g4ZPDk5+YbrlC5dWuPHjyesAgAA/MPZHVxjYmJKog4AAADgum7+9lcAAADALWD3EdfIyMjrXq91w4YNN1UQAAAAUBC7g+vjjz9u8zgnJ0f79+9XYmKiBg0a5LDCAAAAgL+66eCa58MPP9TevXtvuiAAAACgIA6b49qmTRutW7fOUd0BAAAANhwWXNeuXasKFSo4qjsAAADAht1TBbp3725zcpbFYtH58+d14cIFrrUKAACAEmN3cL3//vttHru4uOiOO+7Q3Xffrfr16zusMAAAAOCv7A6uw4cPL4k6AAAAgOviBgQAAAAwhCIfcW3YsOF1bzwg/TltYP/+/TddFAAAAHCtIgfX2bNnF7rsp59+Unx8vHJzcx1SFAAAAHCtIgfXa0/KkqSUlBRNnz5dGzduVNeuXTVy5EiHFgcAAADksfvkLEk6e/asZs2apYSEBLVq1UoJCQkKCAhwdG0AAACAlV3B9dKlS3r33Xe1ZMkSBQYGatGiRWrevHlJ1QYAAABYFTm4zp8/XwsWLFClSpU0ffr0AqcOAAAAACWlyMF1+vTpKlOmjGrVqqWEhAQlJCQUuN71TuICAAAAiqvIwfXaW70CAAAAt1KRg2tsbGxJ1gEAAABcF3fOAgAAgCEQXAEAAGAIBFcAAAAYAsEVwC1j5rbQuA2xXQLGUaw7ZwFAcbi5umpswnKlnD/v7FIASVK9SpU0pXtPZ5cBoIgIrgBuqZTz53XwzGlnlwEAMCCmCgAAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AAAAwBIIrAAAADIHgCgAAAEMguAIAAMAQCK4AANzmci25zi4BsOGsbbKUU0YFAABF5uriqsV7P9CZjLPOLgVQVa8qerzJAKeMTXAFAMAAzmSc1YlLJ5xdBuBUTBUAAACAIRBcAQAAYAgEVwAAABgCwRUAAACGQHAFAACAIRBcAQAAYAgEVwAAABgCwRUAAACGQHAFAACAIRBcAQAAYAgEVwAAABgCwRUAAACGQHAFAACAITg1uO7YsUNDhw5Vq1atZDKZtH79epvlFotFb7/9tlq1aqWmTZtq4MCBOnLkiHOKBQAAgFM5NbhmZGTIZDJp/PjxBS6fP3++4uPjNWHCBH3yySfy9PTUoEGDdPXq1VtcKQAAAJytlDMHb9u2rdq2bVvgMovFog8++EDPPPOM7r//fknS1KlTFR4ervXr16tLly63slQAAAA4mVOD6/WcOHFCqampCg8Pt7Z5e3urWbNm2rVrl93B1Ww2O7Q+Nzc3h/YHOIKjt3NHY7/B7Yp9B7CfI/ebovZ12wbX1NRUSZKvr69Nu6+vr86fP293f3v27HFIXZLk6empRo0aOaw/wFGSk5OVmZnp7DIKxH6D2xn7DmA/Z+w3t21wdbSgoCD+xYq/PZPJ5OwSAENi3wHs58j9xmw2F+kg420bXP38/CRJaWlpqly5srU9LS1NDRs2tLs/Nzc3giv+9tjGgeJh3wHs54z95ra9jmuNGjXk5+enpKQka9vly5f1888/KyQkxImVAQAAwBmcesT1jz/+0LFjx6yPT5w4oQMHDsjHx0fVq1fXgAEDNHfuXNWuXVs1atTQ22+/rcqVK1uvMgAAAIB/DqcG171792rAgAHWxzExMZKkhx9+WLGxsRo8eLAyMzP1yiuvKD09XXfddZcWLFig0qVLO6tkAAAAOIlTg2vLli2VnJxc6HIXFxc999xzeu65525hVQAAALgd3bZzXAEAAIC/IrgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMARDBNcPP/xQkZGRCgoKUu/evbV7925nlwQAAIBb7LYPrqtXr1ZMTIyeffZZrVy5Ug0bNtSgQYOUlpbm7NIAAABwC932wfX9999Xnz591LNnTzVo0EATJ05UmTJltHz5cmeXBgAAgFuolLMLuJ6srCzt27dPQ4YMsba5uroqPDxcu3btKlIfFovF2pebm5vDanNzc1PdWn5yL3XbZ3/8A9So7iuz2Syz2ezsUq7Lzc1NAX5+8nBlv8HtoY6vcfad6p7V5CbHfY8BxVXFs7LD95u8vvJyW2Fu6+D622+/yWw2y9fX16bd19dXKSkpReojNzdXkrR//36H1xfZspakWg7vFyiOn376ydklFMlDNWpLNWo7uwzAyij7TiMFqpFboLPLAKSskttv8nJbYW7r4OoIpUqVUlBQkFxdXeXi4uLscgAAAHANi8Wi3NxclSp1/Wh6WwfXihUrys3NLd+JWGlpaapUqVKR+nB1dZWHh0dJlAcAAIBb6LaeaObh4aHGjRsrKSnJ2pabm6ukpCSFhIQ4sTIAAADcarf1EVdJeuKJJzR69Gg1adJETZs21eLFi5WZmakePXo4uzQAAADcQrd9cO3cubMuXLigmTNnKjU1VYGBgVqwYEGRpwoAAADg78HFcqPrDgAAAAC3gdt6jisAAACQh+AKAAAAQyC4AgAAwBAIrvhb2r59u0wmk9LT051dCuAwRd2uIyMjtWjRoltTFPA3xr50+yG44rqio6NlMpk0b948m/b169fLZDI5bJwTJ07IZDLpwIEDDusTcJa8/cZkMqlJkyZ64IEHNHv2bOXk5NxUvyEhIUpMTJS3t7ckacWKFWrevHm+9T799FP17dv3psYCStqt+n4pCvYl4yC44oZKly6t+fPn6+LFi84uRVlZWc4uASiS1q1bKzExUevWrdMTTzyh2bNna+HChTfVp4eHh/z8/G54++o77rhDnp6eNzUWcCvcTt8vBWFfuv0QXHFD4eHhqlSpkuLi4gpdZ+fOnXr00UfVtGlTtW3bVpMnT1ZGRoZ1uclk0vr1622e07x5c61YsUKSdN9990mSunfvLpPJpKioKEl//ot82LBhmjt3rlq1aqWOHTtKkhISEtSjRw+FhIQoIiJCL730Ur5bAwPOlBcy/f399eijjyo8PFzffPONLl68qH/9619q0aKFmjVrpqeeekpHjhyxPu/kyZMaOnSoWrRooeDgYHXp0kXffvutJNupAtu3b9eYMWN06dIl69HdWbNmSbL9efOll17S888/b1Nbdna2WrZsqYSEBEl/3pEwLi5OkZGRatq0qbp166a1a9eW9FsEOOT75dy5c3r66afVtGlTRUZGatWqVfl+4n///ffVtWtXBQcHq23btpowYYL++OMPSWJfMhiCK27I1dVVL774opYsWaIzZ87kW37s2DENHjxY7du31+eff64ZM2bohx9+0KuvvlrkMf773/9KkhYtWqTExETrHw1JSkpK0uHDh/X+++9b/7jl5OToueee0+eff6533nlHJ0+eVHR09E2+UqDklC5dWtnZ2YqOjtbevXs1d+5cLVu2TBaLRU8//bSys7MlSZMmTVJWVpaWLFmiVatWadSoUfLy8srXX0hIiMaOHaty5copMTFRiYmJevLJJ/Ot17VrV23cuNH6JS1JiYmJunLliu6//35JUlxcnBISEjRx4kR9+eWXGjhwoF5++WV9//33JfRuAH9yxPfL6NGjde7cOcXHx2vWrFn65JNP8h3IcHFx0bhx4/TFF18oNjZW27Zt0xtvvCGJfcloCK4okgceeECBgYGaOXNmvmVxcXHq2rWrBg4cqDp16ig0NFTjxo1TQkKCrl69WqT+77jjDklShQoV5OfnpwoVKliXeXl5afLkybrzzjt15513SpJ69eqltm3bqmbNmgoODta4ceO0efNmmz8owO3AYrHou+++U2JioqpVq6ZvvvlGkydPVvPmzdWwYUNNmzZNZ8+etf4icerUKYWGhspkMqlmzZpq166dWrRoka9fDw8PeXt7y8XFRX5+fvLz81PZsmXzrdeqVSt5enrq66+/trZ98cUXioyMVLly5ZSVlaW4uDhNmTJFrVu3Vs2aNdWjRw9169ZNy5YtK7k3Bvg/N/P9cujQIX333Xd69dVX1axZMzVu3FiTJ0/WlStXbPoZOHCg7rnnHtWoUUNhYWF6/vnntWbNGknsS0Zz29/yFbePUaNG6fHHH9egQYNs2g8ePKjk5GStWrXK2maxWJSbm6sTJ06ofv36NzVuQECAPDw8bNr27t2r2bNn6+DBg7p48aLybgB3+vRpNWjQ4KbGAxxh06ZNCgkJUXZ2tiwWix588EE98MAD2rRpk5o1a2Zdr2LFiqpbt64OHTokSRowYIAmTJigxMREhYeHq3379mrYsGGx6yhVqpQ6deqkVatWqXv37srIyNCGDRv05ptvSpKOHj2qzMzMfEeYsrOzFRgYWOxxAXsU9/vl8OHDKlWqlBo3bmxdXrt2bfn4+Nj089133ykuLk4pKSm6fPmyzGazrl69qszMzCLPYWVfuj0QXFFkLVq0UKtWrTR9+nT16NHD2p6RkaFHHnnEOi/1r6pVqybpz59prr27cFHPsL72j0pGRoYGDRqkVq1aadq0aapYsaJOnz6tQYMGWX9uBZytZcuWmjBhgtzd3VW5cmWVKlVKGzZsuOHzevfurVatWmnTpk3aunWr5s2bp9GjRxe4fxVV165dFRUVpbS0NG3dulWlS5dW69atJck6VzAuLk5VqlSxed61/2AESkpxv18OHz58w75PnDihIUOGqF+/fnrhhRfk4+OjH374QePGjVN2drZdJ1+xLzkfwRV2eemll9S9e3fVrVvX2taoUSP9+uuvql27dqHPu+OOO3Tu3Dnr4yNHjigzM9P62N3dXZJkNptvWENKSop+//13jRo1yhqM9+7da/drAUqSp6dnvn2ifv36ysnJ0c8//6zQ0FBJ0m+//abDhw/b/FJQrVo19evXT/369dP06dP1ySefFPjF7e7uXqR9JjQ0VFWrVtXq1au1efNmdezY0brP1a9fXx4eHjp16pTuvvvum3nJwE0pzvdL3bp1lZOTo/3796tJkyaS/jzy+derFOzbt08Wi0XR0dFydf1zhmTeNIE87EvGwRxX2MVkMqlr166Kj4+3tg0ePFi7du3SpEmTdODAAR05ckTr16/XpEmTrOvcc889+vDDD7V//37t2bNH48ePt+7skuTr66syZcpoy5YtOn/+vC5dulRoDdWrV5e7u7vi4+N1/PhxbdiwQXPmzCmZFww4UJ06dXTffffpP//5j3bu3KmDBw/q5ZdfVpUqVaxX1njttde0ZcsWHT9+XPv27dP27dsLnW7j7++vjIwMJSUl6cKFCzb/GLzWgw8+qKVLl+q7775T165dre3lypXTk08+qZiYGK1cuVLHjh3Tvn37FB8fr5UrVzr2DQCuozjfL/Xr11d4eLheeeUV7d69W/v379d//vMflSlTxnrZuNq1ays7O9v6nZGQkKClS5fajM2+ZBwEV9ht5MiRys3NtT5u2LCh4uPjdeTIET366KN6+OGHNXPmTFWuXNm6zujRo1WtWjX1799fo0aN0pNPPqkyZcpYl5cqVUr//ve/tWzZMrVu3VrDhg0rdPw77rhDsbGxWrt2rTp37qz58+dr9OjRJfNiAQeLiYlR48aNNXToUPXt21cWi0Xz5s2z/kMuNzdXkyZNUufOnfXUU0+pTp06Gj9+fIF9hYaG6pFHHtHzzz+vsLAwLViwoNBxu3Xrpl9//VVVqlTRXXfdZbPs+eef17BhwxQXF2cdd9OmTapRo4bjXjhQBMX5fnn99dfl6+ur/v37a/jw4erTp4/Kli2r0qVLW/sYM2aM5s+frwcffFCrVq3Siy++aDMu+5JxuFiunXgIAABgUGfOnFHbtm21aNEihYWFObscOBhzXAEAgGElJSUpIyNDAQEBSk1N1RtvvCF/f/8Cb+EK4yO4AgAAw8rJydGMGTN0/PhxlS1bViEhIZo2bZrNeRT4+2CqAAAAAAyBk7MAAABgCARXAAAAGALBFQAAAIZAcAUAAIAhEFwBAABgCARXADCA7du3y2QyKT093dmlAIDTcDksALDDhQsX9Pbbb+vbb7/V+fPn5ePjo4YNG2rYsGH5bv9YXFFRUWrYsKHGjRtnbcvKytLFixdVqVIl6z3YnSU6Olrp6emaM2eOU+sA8M/DDQgAwA4jRoxQdna2YmNjVbNmTaWlpSkpKUm///57iY7r4eEhPz+/Eh0DAG53TBUAgCJKT0/Xzp07NWrUKN1zzz3y9/dX06ZNNWTIEN13333WdcaNG6d77rlHoaGhGjBggA4ePGjtY9asWXrooYeUkJCgyMhI3XXXXXrhhRd0+fJlSX8ezfz+++/1wQcfyGQyyWQy6cSJE/mmCqxYsULNmzfXxo0b1aFDBzVr1kwjR45UZmamVq5cqcjISLVo0UKTJ0+W2Wy2jp+VlaXXX39drVu3VnBwsHr37q3t27dbl+f1u2XLFnXq1EkhISEaNGiQzp07Z61/5cqV2rBhg7W+vz4fAEoSwRUAisjLy0teXl5av369srKyClznueeeU1pamubPn68VK1aocePGevzxx22OyB47dkwbNmzQu+++q7i4OO3YsUPz58+XJI0bN04hISHq06ePEhMTlZiYqGrVqhU41pUrVxQfH68ZM2ZowYIF2r59u4YPH65vv/1W8+bN09SpU7V06VKtW7fO+pxJkyZp165dmjFjhj7//HN17NhRTz31lI4cOWLT73vvvaepU6dqyZIlOn36tF5//XVJ0pNPPqlOnTqpdevW1vpCQkJu8p0FgKJhqgAAFFGpUqUUGxur//znP1q6dKkaNWqku+++W507d1bDhg21c+dO7d69W0lJSfLw8JAkjR49WuvXr9e6devUt29fSZLFYlFMTIzKlSsnSerWrZuSkpL0wgsvyNvbW+7u7ipTpswNpwZkZ2drwoQJqlWrliSpQ4cO+vzzz7V161aVLVtWDRo0UMuWLbVt2zZ17txZp06d0ooVK7Rx40ZVqVJFkjRo0CBt2bJFK1as0Isvvmjtd+LEidZ++/fvb53PWrZsWZUpU0ZZWVlMXQBwyxFcAcAOHTp00L333qudO3fqp59+0pYtW7RgwQJNnjxZmZmZysjIUMuWLW2ec+XKFR07dsz62N/f3xpaJaly5cpKS0uzuxZPT09ruJSkSpUqyd/fX2XLlrVpu3DhgiTpf//7n8xmszp27GjTT1ZWlipUqFBov8WtDwAcjeAKAHYqXbq0IiIiFBERoWeffVbjxo3TrFmz1K9fP/n5+Sk+Pj7fc7y9va3/X6pU/j+9xbnAy7X9uLi4FNiWm5srScrIyJCbm5uWL18uNzc3m/W8vLyu2y8XoAFwOyC4AsBNatCggdavX6/GjRvr/PnzcnNzU40aNYrdn7u7uzVsOlJgYKDMZrMuXLig5s2bF7ufkqoPAG6Ek7MAoIh+++03DRgwQJ999pkOHjyo48ePa82aNVqwYIHuu+8+hYeHKzg4WM8++6wSExN14sQJ/fjjj5oxY4b27NlT5HH8/f31888/68SJE7pw4YLDQmLdunXVtWtX/etf/9JXX32l48ePa/fu3YqLi9OmTZvsqi85OVkpKSm6cOGCsrOzHVIfANwIR1wBoIjKli2rZs2aafHixTp27JhycnJUtWpV9e7dW0OHDpWLi4vmzZunt956S2PGjNFvv/2mSpUqqXnz5qpUqVKRx3nyyScVHR2tLl266MqVK9qwYYPDXkNMTIzmzp2r2NhYnTt3ThUqVFBwcLDuvffeIvfRp08fff/99+rZs6cyMjL0wQcf5JvXCwAlgTtnAQAAwBCYKgAAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMASCKwAAAAyB4AoAAABDILgCAADAEAiuAAAAMIT/B/xJGX4KZtb7AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19f60df2",
        "outputId": "f7f23203-f5cd-42d0-f3b1-9438409b5751"
      },
      "source": [
        "from textblob import TextBlob # Import the TextBlob class for sentiment analysis\n",
        "\n",
        "# Function to get sentiment from a given text\n",
        "def get_sentiment(text): # Define a function named 'get_sentiment' that takes a 'text' argument\n",
        "    if isinstance(text, str): # Check if the input 'text' is a string\n",
        "        analysis = TextBlob(text) # Create a TextBlob object for the input text\n",
        "        if analysis.sentiment.polarity > 0: # If the polarity score is greater than 0\n",
        "            return 'Positive' # Classify the sentiment as 'Positive'\n",
        "        elif analysis.sentiment.polarity < 0: # If the polarity score is less than 0\n",
        "            return 'Negative' # Classify the sentiment as 'Negative'\n",
        "        else: # If the polarity score is exactly 0\n",
        "            return 'Neutral' # Classify the sentiment as 'Neutral'\n",
        "    return None # If the input is not a string, return None\n",
        "\n",
        "# Apply sentiment analysis to the 'Issues' column of the DataFrame\n",
        "df['Polarity'] = df['Issues'].apply(lambda x: TextBlob(str(x)).sentiment.polarity) # Calculate the polarity score for each issue and store it in a new 'Polarity' column\n",
        "df['Sentiment'] = df['Issues'].apply(get_sentiment) # Apply the 'get_sentiment' function to each issue and store the result in a new 'Sentiment' column\n",
        "\n",
        "print(\"Sentiment analysis complete. Added 'Polarity' and 'Sentiment' columns.\") # Print a message indicating completion\n",
        "print(\"\\nDataFrame with Sentiment:\") # Print a header for the DataFrame snippet\n",
        "print(df[['Issues', 'Polarity', 'Sentiment']].head()) # Display the first 5 rows of 'Issues', 'Polarity', and 'Sentiment' columns\n",
        "\n",
        "print(\"\\nSentiment Distribution:\") # Print a header for the sentiment distribution\n",
        "print(df['Sentiment'].value_counts()) # Print the count of each sentiment category"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis complete. Added 'Polarity' and 'Sentiment' columns.\n",
            "\n",
            "DataFrame with Sentiment:\n",
            "                                              Issues  Polarity Sentiment\n",
            "0               Object reference error while AI scan      0.00   Neutral\n",
            "1  For Barcode scanning, the response shows as  '...      0.00   Neutral\n",
            "2  DOT format is 13 and 11 digits not 13 and 12 d...      0.00   Neutral\n",
            "3  Need to amend the duplicate image message pop ...     -0.15  Negative\n",
            "4  Tire data is not getting resgitered while scan...      0.40  Positive\n",
            "\n",
            "Sentiment Distribution:\n",
            "Sentiment\n",
            "Neutral     45\n",
            "Positive    17\n",
            "Negative    12\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb8f6ad6"
      },
      "source": [
        "# Task\n",
        "Perform topic modeling on the 'Issues' column to identify common themes and categories within customer feedback, specifically by preprocessing the text data and applying TF-IDF vectorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0beb47b4"
      },
      "source": [
        "## Topic Modeling/Clustering of Issues\n",
        "\n",
        "### Subtask:\n",
        "Preprocess the text data in the 'Issues' column, apply TF-IDF vectorization, and then perform topic modeling using Latent Dirichlet Allocation (LDA) to identify common themes within customer feedback.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41176e27"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the necessary libraries for text preprocessing and topic modeling, and ensure that the required NLTK data (stopwords and wordnet) is downloaded. This sets up the environment for the subsequent steps of cleaning the text data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "449f47ef",
        "outputId": "fe38adad-4ccc-4578-c7fb-7049abd55464"
      },
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "print(\"Downloading NLTK stopwords and wordnet if not already present...\")\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "print(\"Required libraries imported and NLTK data downloaded.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK stopwords and wordnet if not already present...\n",
            "Required libraries imported and NLTK data downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac52e0c8"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary libraries and NLTK data are imported and downloaded, the next step is to define and apply a text preprocessing function to clean the 'Issues' column, as outlined in instructions 3 and 4 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b850b12"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the NLTK 'punkt' tokenizer data was not downloaded, which is required by `word_tokenize`. I will modify the previous code block to explicitly download 'punkt' along with stopwords and wordnet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d478dd9",
        "outputId": "78beddde-d614-481a-899b-b90cc28167cd"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download punkt tokenizer if not already present\n",
        "print(\"Downloading NLTK 'punkt' tokenizer and 'punkt_tab' if not already present...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "print(\"'punkt' and 'punkt_tab' tokenizer download complete.\")\n",
        "\n",
        "# Initialize NLTK components\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and lemmatize\n",
        "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    # Join back to string\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "# Apply preprocessing to the 'Issues' column\n",
        "df['Cleaned_Issues'] = df['Issues'].apply(preprocess_text)\n",
        "\n",
        "print(\"Text preprocessing complete. 'Cleaned_Issues' column added.\")\n",
        "print(df[['Issues', 'Cleaned_Issues']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK 'punkt' tokenizer and 'punkt_tab' if not already present...\n",
            "'punkt' and 'punkt_tab' tokenizer download complete.\n",
            "Text preprocessing complete. 'Cleaned_Issues' column added.\n",
            "                                              Issues  \\\n",
            "0               Object reference error while AI scan   \n",
            "1  For Barcode scanning, the response shows as  '...   \n",
            "2  DOT format is 13 and 11 digits not 13 and 12 d...   \n",
            "3  Need to amend the duplicate image message pop ...   \n",
            "4  Tire data is not getting resgitered while scan...   \n",
            "\n",
            "                                      Cleaned_Issues  \n",
            "0                     object reference error ai scan  \n",
            "1  barcode scanning response show image produce r...  \n",
            "2                             dot format digit digit  \n",
            "3  need amend duplicate image message pop current...  \n",
            "4  tire data getting resgitered scanning tire sid...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24ca7356"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the text data has been preprocessed, the next step is to apply TF-IDF vectorization to convert the cleaned text into a numerical format that can be used for topic modeling, as per instructions 5 and 6 of the subtask. I will initialize a `TfidfVectorizer` with `max_df` and `min_df` parameters to filter out common and rare words, then fit and transform the `Cleaned_Issues` column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71eced8c",
        "outputId": "81d33b98-d0ff-422a-ca48-c5d41168ca5a"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "\n",
        "# Fit and transform the cleaned issues to create the TF-IDF matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Cleaned_Issues'])\n",
        "\n",
        "print(\"TF-IDF vectorization complete. TF-IDF matrix created with shape:\", tfidf_matrix.shape)\n",
        "print(\"Number of features (unique words):\", len(tfidf_vectorizer.get_feature_names_out()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF vectorization complete. TF-IDF matrix created with shape: (74, 94)\n",
            "Number of features (unique words): 94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9276a037"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the TF-IDF matrix has been created, the next step is to perform topic modeling using Latent Dirichlet Allocation (LDA), as specified in instructions 7 and 8 of the subtask. I will initialize an LDA model, fit it to the TF-IDF matrix, and then print the top keywords for each identified topic to reveal common themes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "577bde88",
        "outputId": "a637660c-1fbd-4554-d525-8b9931b98c36"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Initialize LatentDirichletAllocation model\n",
        "n_components = 5 # A good starting point for the number of topics\n",
        "lda_model = LatentDirichletAllocation(n_components=n_components, random_state=42)\n",
        "\n",
        "# Fit the LDA model to the TF-IDF matrix\n",
        "lda_output = lda_model.fit_transform(tfidf_matrix)\n",
        "\n",
        "print(\"LDA model fitting complete. Extracted topics.\")\n",
        "\n",
        "# Function to print top words for each topic\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_): # Corrected from components__ to components_\n",
        "        print(f\"Topic #{topic_idx + 1}:\")\n",
        "        print(\" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "    print()\n",
        "\n",
        "# Print the top words for each topic\n",
        "n_top_words = 10 # Number of top words to display per topic\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print_top_words(lda_model, tfidf_feature_names, n_top_words)\n",
        "\n",
        "print(\"Topic modeling complete. Displaying top keywords for each topic.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA model fitting complete. Extracted topics.\n",
            "Topic #1:\n",
            "claim user action queue showing auto working approve product clicking\n",
            "Topic #2:\n",
            "dot getting add data new image serial wall fetched pattern\n",
            "Topic #3:\n",
            "message claim dcms aug new creation approval issue need sending\n",
            "Topic #4:\n",
            "barcode image app team sidewall scan shared video scanning result\n",
            "Topic #5:\n",
            "bring origin based digit dot check anyline duplicate ai format\n",
            "\n",
            "Topic modeling complete. Displaying top keywords for each topic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53af108a"
      },
      "source": [
        "## Feature Engineering for Predictive Modeling\n",
        "\n",
        "### Subtask:\n",
        "Based on the existing data and newly extracted topics, create relevant features that could be used for predictive modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05a3a26d"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to add the topic distribution from the `lda_output` (obtained from LDA topic modeling) as new columns to the DataFrame `df`. Each column will represent the probability of an issue belonging to a specific topic, as specified in instruction 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1e394d8",
        "outputId": "ba63ede5-a4c1-4b6d-a721-b326ec3cbf69"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assign topic probabilities to new columns in df\n",
        "for i in range(n_components):\n",
        "    df[f'topic_{i+1}_prob'] = lda_output[:, i]\n",
        "\n",
        "print(\"Topic probabilities added to DataFrame.\")\n",
        "print(df[[f'topic_{i+1}_prob' for i in range(n_components)]].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic probabilities added to DataFrame.\n",
            "   topic_1_prob  topic_2_prob  topic_3_prob  topic_4_prob  topic_5_prob\n",
            "0      0.073815      0.074095      0.074564      0.702920      0.074607\n",
            "1      0.062765      0.063057      0.062650      0.748914      0.062614\n",
            "2      0.078083      0.079734      0.077622      0.078402      0.686160\n",
            "3      0.064151      0.065050      0.741801      0.064730      0.064269\n",
            "4      0.049529      0.116178      0.049904      0.734257      0.050132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9f64be4"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the topic probabilities have been added, the next step is to create two new features: 'issue_length' by calculating the character length of the 'Issues' column, and 'word_count' by counting the words in the 'Issues' column, as specified in instructions 2 and 3 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6db24ca",
        "outputId": "787c91c6-5c99-417f-bb80-753b575f442c"
      },
      "source": [
        "df['issue_length'] = df['Issues'].astype(str).apply(len)\n",
        "df['word_count'] = df['Issues'].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "print(\"Created 'issue_length' and 'word_count' features.\")\n",
        "print(df[['Issues', 'issue_length', 'word_count']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 'issue_length' and 'word_count' features.\n",
            "                                              Issues  issue_length  word_count\n",
            "0               Object reference error while AI scan            36           6\n",
            "1  For Barcode scanning, the response shows as  '...            77          13\n",
            "2  DOT format is 13 and 11 digits not 13 and 12 d...            51          12\n",
            "3  Need to amend the duplicate image message pop ...            79          14\n",
            "4  Tire data is not getting resgitered while scan...           168          28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac553c71"
      },
      "source": [
        "**Reasoning**:\n",
        "The new features 'issue_length' and 'word_count' have been created, along with the topic probability columns. The next step, as specified in instruction 4 of the subtask, is to verify their creation by displaying the head of the DataFrame with these new columns and providing descriptive statistics for the numerical features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b09208a0",
        "outputId": "9c6acda7-0173-4b1d-ad8e-0358c9332547"
      },
      "source": [
        "print(\"Verifying new features and displaying descriptive statistics:\")\n",
        "print(df[[f'topic_{i+1}_prob' for i in range(n_components)] + ['issue_length', 'word_count']].head())\n",
        "\n",
        "print(\"\\nDescriptive statistics for new numerical features:\")\n",
        "print(df[[f'topic_{i+1}_prob' for i in range(n_components)] + ['issue_length', 'word_count']].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying new features and displaying descriptive statistics:\n",
            "   topic_1_prob  topic_2_prob  topic_3_prob  topic_4_prob  topic_5_prob  \\\n",
            "0      0.073815      0.074095      0.074564      0.702920      0.074607   \n",
            "1      0.062765      0.063057      0.062650      0.748914      0.062614   \n",
            "2      0.078083      0.079734      0.077622      0.078402      0.686160   \n",
            "3      0.064151      0.065050      0.741801      0.064730      0.064269   \n",
            "4      0.049529      0.116178      0.049904      0.734257      0.050132   \n",
            "\n",
            "   issue_length  word_count  \n",
            "0            36           6  \n",
            "1            77          13  \n",
            "2            51          12  \n",
            "3            79          14  \n",
            "4           168          28  \n",
            "\n",
            "Descriptive statistics for new numerical features:\n",
            "       topic_1_prob  topic_2_prob  topic_3_prob  topic_4_prob  topic_5_prob  \\\n",
            "count     74.000000     74.000000     74.000000     74.000000     74.000000   \n",
            "mean       0.235081      0.264030      0.170648      0.196902      0.133340   \n",
            "std        0.272333      0.294503      0.241861      0.256903      0.194816   \n",
            "min        0.049529      0.047937      0.044906      0.043817      0.043840   \n",
            "25%        0.062859      0.065254      0.062141      0.062611      0.058958   \n",
            "50%        0.073857      0.074644      0.070989      0.068417      0.067275   \n",
            "75%        0.391326      0.644226      0.084240      0.085823      0.078398   \n",
            "max        0.802672      0.777901      0.806200      0.795069      0.774291   \n",
            "\n",
            "       issue_length  word_count  \n",
            "count     74.000000   74.000000  \n",
            "mean      75.459459   13.175676  \n",
            "std       41.792111    7.578553  \n",
            "min       15.000000    2.000000  \n",
            "25%       48.000000    8.000000  \n",
            "50%       65.500000   12.000000  \n",
            "75%       94.250000   17.000000  \n",
            "max      222.000000   40.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1caa55be"
      },
      "source": [
        "## Predictive Model for Issue Priority/Severity\n",
        "\n",
        "### Subtask:\n",
        "Prepare the target variable for predictive modeling by defining 'priority' or 'severity' based on available data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "740c38ff"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in preparing the target variable for predictive modeling is to review the available columns in the DataFrame to identify potential candidates for 'priority' or 'severity'. Displaying the column names, unique values, and their counts for relevant columns like 'Status' and 'Type' will help in making an informed decision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87f9b50c",
        "outputId": "3d493a6c-f02c-4a52-b7e6-46683c505cd7"
      },
      "source": [
        "print(\"Available columns in the DataFrame:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "print(\"\\nUnique values and counts for 'Status BS' column:\")\n",
        "print(df['Status BS'].value_counts(dropna=False))\n",
        "\n",
        "print(\"\\nUnique values and counts for 'Type' column:\")\n",
        "print(df['Type'].value_counts(dropna=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available columns in the DataFrame:\n",
            "['Sl.No.', 'Reported Date', 'Reported By', 'Issues', 'Mode', 'Type', 'Status BS', 'Polarity', 'Sentiment', 'Cleaned_Issues', 'topic_1_prob', 'topic_2_prob', 'topic_3_prob', 'topic_4_prob', 'topic_5_prob', 'issue_length', 'word_count']\n",
            "\n",
            "Unique values and counts for 'Status BS' column:\n",
            "Status BS\n",
            "Completed        36\n",
            "NaN              29\n",
            "Anyline issue     7\n",
            "completed         2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Unique values and counts for 'Type' column:\n",
            "Type\n",
            "NaN            44\n",
            "Phase 1        23\n",
            "Phase1          5\n",
            "Phase 1-Bug     2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0a6eb71"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the review of the 'Status BS' and 'Type' columns, I will first clean these columns by unifying similar categories and then create a new 'severity' target variable. This variable will be categorized as 'High' for critical issues ('Phase 1-Bug' or 'Anyline issue'), 'Low' for resolved issues ('Completed'), and 'Medium' for all other cases, including currently undefined or generic issues. This directly addresses instruction 2 and 3 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58f1b2b7",
        "outputId": "13eda411-654f-4344-e7f5-6ae491aacd33"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Clean 'Status BS' column\n",
        "df['Status BS'] = df['Status BS'].replace({'completed': 'Completed'})\n",
        "\n",
        "# Clean 'Type' column\n",
        "df['Type'] = df['Type'].replace({'Phase1': 'Phase 1'})\n",
        "\n",
        "# Create a new 'Severity' column based on 'Status BS' and 'Type'\n",
        "def assign_severity(row):\n",
        "    if row['Type'] == 'Phase 1-Bug' or row['Status BS'] == 'Anyline issue':\n",
        "        return 'High'\n",
        "    elif row['Status BS'] == 'Completed':\n",
        "        return 'Low'\n",
        "    else:\n",
        "        return 'Medium' # Includes NaN in Type, Phase 1, and NaN in Status BS\n",
        "\n",
        "df['Severity'] = df.apply(assign_severity, axis=1)\n",
        "\n",
        "print(\"Cleaned 'Status BS' and 'Type' columns, and created 'Severity' target variable.\")\n",
        "print(\"\\nValue counts for the new 'Severity' column:\")\n",
        "print(df['Severity'].value_counts())\n",
        "print(\"\\nFirst 5 rows with new 'Severity' column:\")\n",
        "print(df[['Issues', 'Status BS', 'Type', 'Severity']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned 'Status BS' and 'Type' columns, and created 'Severity' target variable.\n",
            "\n",
            "Value counts for the new 'Severity' column:\n",
            "Severity\n",
            "Low       36\n",
            "Medium    29\n",
            "High       9\n",
            "Name: count, dtype: int64\n",
            "\n",
            "First 5 rows with new 'Severity' column:\n",
            "                                              Issues  Status BS         Type  \\\n",
            "0               Object reference error while AI scan  Completed  Phase 1-Bug   \n",
            "1  For Barcode scanning, the response shows as  '...  Completed  Phase 1-Bug   \n",
            "2  DOT format is 13 and 11 digits not 13 and 12 d...  Completed      Phase 1   \n",
            "3  Need to amend the duplicate image message pop ...  Completed      Phase 1   \n",
            "4  Tire data is not getting resgitered while scan...        NaN      Phase 1   \n",
            "\n",
            "  Severity  \n",
            "0     High  \n",
            "1     High  \n",
            "2      Low  \n",
            "3      Low  \n",
            "4   Medium  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3671d86"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the 'Severity' target variable has been created, the next step is to preprocess it for predictive modeling. This involves encoding the categorical 'Severity' column into numerical values using a technique like Label Encoding, as specified in instruction 4 of the subtask. I will also verify the distribution of the encoded target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16ea0b5e",
        "outputId": "922fea18-7533-4e83-c7c9-47f298dd3523"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Encode the 'Severity' column\n",
        "df['Severity_encoded'] = le.fit_transform(df['Severity'])\n",
        "\n",
        "print(\"Encoded 'Severity' column into numerical values.\")\n",
        "print(\"Mapping of original Severity to encoded values:\")\n",
        "for i, severity_label in enumerate(le.classes_):\n",
        "    print(f\"{severity_label}: {i}\")\n",
        "\n",
        "print(\"\\nFirst 5 rows with new 'Severity_encoded' column:\")\n",
        "print(df[['Severity', 'Severity_encoded']].head())\n",
        "\n",
        "print(\"\\nValue counts for the new 'Severity_encoded' column:\")\n",
        "print(df['Severity_encoded'].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 'Severity' column into numerical values.\n",
            "Mapping of original Severity to encoded values:\n",
            "High: 0\n",
            "Low: 1\n",
            "Medium: 2\n",
            "\n",
            "First 5 rows with new 'Severity_encoded' column:\n",
            "  Severity  Severity_encoded\n",
            "0     High                 0\n",
            "1     High                 0\n",
            "2      Low                 1\n",
            "3      Low                 1\n",
            "4   Medium                 2\n",
            "\n",
            "Value counts for the new 'Severity_encoded' column:\n",
            "Severity_encoded\n",
            "1    36\n",
            "2    29\n",
            "0     9\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "357a8354"
      },
      "source": [
        "## Anomaly Detection for Unusual Issues\n",
        "\n",
        "### Subtask:\n",
        "Implement an anomaly detection model (e.g., Isolation Forest or One-Class SVM) on the issue descriptions or extracted features to identify highly unusual or novel issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab6a8ee5"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to select the numerical features for anomaly detection as specified in instruction 1, which include the topic probabilities, issue length, and word count. These features will be extracted from the DataFrame and stored in a new variable, `features_for_anomaly`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef70a60c",
        "outputId": "a8dd7a05-d12a-4a17-b86d-8735222298d2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Select the numerical features for anomaly detection\n",
        "features_for_anomaly = df[[f'topic_{i+1}_prob' for i in range(n_components)] + ['issue_length', 'word_count']]\n",
        "\n",
        "# Check for any potential missing values and handle them (e.g., fill with median or mean if present)\n",
        "# Based on previous describe output, these columns do not have NaNs, but it's good practice to ensure.\n",
        "if features_for_anomaly.isnull().sum().sum() > 0:\n",
        "    print(\"Warning: Missing values found in features for anomaly detection. Filling with median.\")\n",
        "    features_for_anomaly = features_for_anomaly.fillna(features_for_anomaly.median())\n",
        "\n",
        "print(\"Selected features for anomaly detection:\")\n",
        "print(features_for_anomaly.head())\n",
        "print(f\"Shape of features for anomaly detection: {features_for_anomaly.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features for anomaly detection:\n",
            "   topic_1_prob  topic_2_prob  topic_3_prob  topic_4_prob  topic_5_prob  \\\n",
            "0      0.073815      0.074095      0.074564      0.702920      0.074607   \n",
            "1      0.062765      0.063057      0.062650      0.748914      0.062614   \n",
            "2      0.078083      0.079734      0.077622      0.078402      0.686160   \n",
            "3      0.064151      0.065050      0.741801      0.064730      0.064269   \n",
            "4      0.049529      0.116178      0.049904      0.734257      0.050132   \n",
            "\n",
            "   issue_length  word_count  \n",
            "0            36           6  \n",
            "1            77          13  \n",
            "2            51          12  \n",
            "3            79          14  \n",
            "4           168          28  \n",
            "Shape of features for anomaly detection: (74, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d2935c6"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the features for anomaly detection have been prepared, the next step is to import the IsolationForest model, initialize it, fit it to the `features_for_anomaly`, predict the anomaly labels, add them to the DataFrame, and then display the counts and top anomalous issues, as specified in instructions 2 through 8 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0de16307",
        "outputId": "cc197274-4a38-4028-aed9-8ddd7ea5c125"
      },
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Initialize IsolationForest model\n",
        "# contamination='auto' will estimate the proportion of outliers in the data\n",
        "# random_state for reproducibility\n",
        "isolation_forest = IsolationForest(random_state=42, contamination='auto')\n",
        "\n",
        "# Fit the IsolationForest model to the selected features\n",
        "isolation_forest.fit(features_for_anomaly)\n",
        "\n",
        "# Predict anomaly labels (-1 for anomalies, 1 for inliers)\n",
        "df['anomaly_label'] = isolation_forest.predict(features_for_anomaly)\n",
        "\n",
        "print(\"Isolation Forest model fitted and anomaly labels predicted.\")\n",
        "\n",
        "# Print the count of anomalies and inliers\n",
        "print(\"\\nDistribution of Anomaly Labels:\")\n",
        "print(df['anomaly_label'].value_counts())\n",
        "\n",
        "# Display the 'Issues' column along with the 'anomaly_label' for the top 5 anomalous issues\n",
        "print(\"\\nTop 5 Detected Anomalous Issues:\")\n",
        "print(df[df['anomaly_label'] == -1][['Issues', 'anomaly_label']].head())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Isolation Forest model fitted and anomaly labels predicted.\n",
            "\n",
            "Distribution of Anomaly Labels:\n",
            "anomaly_label\n",
            " 1    55\n",
            "-1    19\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Top 5 Detected Anomalous Issues:\n",
            "                                               Issues  anomaly_label\n",
            "4   Tire data is not getting resgitered while scan...             -1\n",
            "12                                Barcode Not Reading             -1\n",
            "21  '.'' needs to be replaced while considering th...             -1\n",
            "24  even if anyline didnt produce results, the sid...             -1\n",
            "31  Multiple Origins showing in origin dropdown af...             -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96a33f59"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings from topic modeling, present the predictive capabilities of the models, and outline how these insights can be leveraged to enhance business value and proactively address customer issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0e115c0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Topic Identification**: Five distinct topics were identified from customer issues using LDA, characterized by keywords such as 'claim user action queue', 'dot getting add data', 'message claim dcms aug', 'barcode image app team', and 'bring origin based digit'.\n",
        "*   **Feature Engineering**:\n",
        "    *   Topic probabilities were successfully extracted, with five new columns (`topic_1_prob` to `topic_5_prob`) showing varying distributions (e.g., probabilities from approximately 0.04 to 0.81).\n",
        "    *   Text-based features were created: `issue_length` (average 75 characters, range 15 to 222) and `word_count` (average 13 words, range 2 to 40).\n",
        "*   **Target Variable Creation**: A 'Severity' target variable was successfully engineered and encoded, categorizing issues into 'High' (9 issues), 'Low' (36 issues), and 'Medium' (29 issues) based on 'Status BS' and 'Type' columns. The encoded mapping assigned 'High': 0, 'Low': 1, and 'Medium': 2.\n",
        "*   **Anomaly Detection**: An Isolation Forest model identified 19 issues as anomalies and 55 as normal ('inliers') within the dataset, using the engineered topic probabilities, issue length, and word count features.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The identified topics provide a structured way to categorize and understand common customer feedback themes, which can be used to streamline support, improve product features, or refine communication strategies.\n",
        "*   The engineered features and the 'Severity\\_encoded' target variable are now ready for training a predictive model to automatically classify the severity of new issues, enabling proactive resource allocation and faster response times for critical problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4x3o7UWsnHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da4251df"
      },
      "source": [
        "# Task\n",
        "Develop a GenAI-powered agent to optimize tire loading for containers/trucks, encompassing data acquisition and preprocessing, machine learning model development for fit prediction, optimization algorithms for space utilization, a what-if scenario simulation engine, real-time estimation capabilities, and integration with the B-Connect system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43630ad0"
      },
      "source": [
        "## Data Acquisition and Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Define and implement methods to automatically pull historical loading data from various sources (databases, Excel files). This includes data cleaning, transformation, and preparing the data for subsequent machine learning and optimization tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "593ddef4"
      },
      "source": [
        "Please upload any relevant historical loading data files (e.g., CSV, Excel files) to the Colab environment. You can do this by clicking the folder icon on the left panel, then clicking the 'Upload to session storage' icon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2133419b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the user has been instructed to upload files, the next step is to provide Python code to load these files (CSV or Excel) into pandas DataFrames, as specified in instruction 2 of the subtask. This code will attempt to load two hypothetical files, `historical_loading_data.csv` and `tire_specifications.xlsx`, handling potential `FileNotFoundError` for each.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cbb82d8",
        "outputId": "d3ea78ad-61c0-4a2b-90be-dfa33e3c3dc3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Attempt to load historical loading data from a CSV file\n",
        "try:\n",
        "    df_loading_data = pd.read_csv('historical_loading_data.csv')\n",
        "    print(\"\\'historical_loading_data.csv\\' loaded successfully!\")\n",
        "    print(\"First 5 rows of df_loading_data:\")\n",
        "    print(df_loading_data.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: \\'historical_loading_data.csv\\' not found. Please ensure the file is uploaded correctly.\")\n",
        "    df_loading_data = pd.DataFrame() # Initialize empty DataFrame to avoid errors later\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading historical_loading_data.csv: {e}\")\n",
        "    df_loading_data = pd.DataFrame() # Initialize empty DataFrame to avoid errors later\n",
        "\n",
        "# Attempt to load tire specifications from an Excel file\n",
        "try:\n",
        "    df_tire_specs = pd.read_excel('tire_specifications.xlsx')\n",
        "    print(\"\\n\\'tire_specifications.xlsx\\' loaded successfully!\")\n",
        "    print(\"First 5 rows of df_tire_specs:\")\n",
        "    print(df_tire_specs.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: \\'tire_specifications.xlsx\\' not found. Please ensure the file is uploaded correctly.\")\n",
        "    df_tire_specs = pd.DataFrame() # Initialize empty DataFrame to avoid errors later\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading tire_specifications.xlsx: {e}\")\n",
        "    df_tire_specs = pd.DataFrame() # Initialize empty DataFrame to avoid errors later"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'historical_loading_data.csv' not found. Please ensure the file is uploaded correctly.\n",
            "Error: 'tire_specifications.xlsx' not found. Please ensure the file is uploaded correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a17d2bdf"
      },
      "source": [
        "No data was loaded into `df_loading_data` or `df_tire_specs` because the files `historical_loading_data.csv` and `tire_specifications.xlsx` were not found.\n",
        "\n",
        "Please ensure you have uploaded these files to the Colab environment. Once uploaded, please re-run the previous code cell to load the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16b95887"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the user has been reminded to upload the files and re-run the loading code, the next logical step, as per instruction 3 of the subtask, is to perform initial data exploration on the `df_loading_data` and `df_tire_specs` DataFrames. This involves displaying the first few rows, checking data types and non-null values, and getting descriptive statistics, but only if the DataFrames are not empty.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e0a1467",
        "outputId": "e104c806-f871-4cfd-a52b-8f62f4009a9a"
      },
      "source": [
        "print(\"\\n--- Initial Data Exploration for df_loading_data ---\")\n",
        "if not df_loading_data.empty:\n",
        "    print(\"First 5 rows of df_loading_data:\")\n",
        "    print(df_loading_data.head())\n",
        "    print(\"\\nInfo for df_loading_data:\")\n",
        "    df_loading_data.info()\n",
        "    print(\"\\nDescriptive statistics for df_loading_data:\")\n",
        "    print(df_loading_data.describe(include='all'))\n",
        "else:\n",
        "    print(\"df_loading_data is empty. Please ensure 'historical_loading_data.csv' was uploaded and loaded successfully.\")\n",
        "\n",
        "print(\"\\n--- Initial Data Exploration for df_tire_specs ---\")\n",
        "if not df_tire_specs.empty:\n",
        "    print(\"First 5 rows of df_tire_specs:\")\n",
        "    print(df_tire_specs.head())\n",
        "    print(\"\\nInfo for df_tire_specs:\")\n",
        "    df_tire_specs.info()\n",
        "    print(\"\\nDescriptive statistics for df_tire_specs:\")\n",
        "    print(df_tire_specs.describe(include='all'))\n",
        "else:\n",
        "    print(\"df_tire_specs is empty. Please ensure 'tire_specifications.xlsx' was uploaded and loaded successfully.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Initial Data Exploration for df_loading_data ---\n",
            "df_loading_data is empty. Please ensure 'historical_loading_data.csv' was uploaded and loaded successfully.\n",
            "\n",
            "--- Initial Data Exploration for df_tire_specs ---\n",
            "df_tire_specs is empty. Please ensure 'tire_specifications.xlsx' was uploaded and loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fa2c031"
      },
      "source": [
        "It appears that `df_loading_data` and `df_tire_specs` are still empty. This means the files `historical_loading_data.csv` and `tire_specifications.xlsx` have not been successfully loaded yet.\n",
        "\n",
        "Please ensure you have uploaded these files to the Colab environment using the folder icon on the left panel, and then **re-run the code cells for data loading (the one containing `pd.read_csv` and `pd.read_excel`)** before proceeding. Once the data is loaded, you can re-run the exploration cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e6539fe"
      },
      "source": [
        "## Data Acquisition and Preprocessing (Retry)\n",
        "\n",
        "### Subtask:\n",
        "Define and implement methods to automatically pull historical loading data from various sources (databases, Excel files). This includes data cleaning, transformation, and preparing the data for subsequent machine learning and optimization tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db8df613"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to load the historical loading data and tire specifications into pandas DataFrames. The previous attempt failed due to missing files. This step will re-attempt to load the files, assuming the user has now uploaded them, and display the first few rows of each DataFrame as specified in instructions 2 and 3 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20dfc70b",
        "outputId": "66457361-0cb8-4b60-870f-840aa41b61ff"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Attempt to load historical loading data from a CSV file\n",
        "try:\n",
        "    df_loading_data = pd.read_csv('historical_loading_data.csv')\n",
        "    print(\"'historical_loading_data.csv' loaded successfully!\")\n",
        "    print(\"First 5 rows of df_loading_data:\")\n",
        "    print(df_loading_data.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'historical_loading_data.csv' not found. Please ensure the file is uploaded correctly.\")\n",
        "    df_loading_data = pd.DataFrame() # Initialize empty DataFrame to avoid errors later\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading historical_loading_data.csv: {e}\")\n",
        "    df_loading_data = pd.DataFrame() # Initialize empty DataFrame to avoid errors later\n",
        "\n",
        "# Attempt to load tire specifications from an Excel file\n",
        "try:\n",
        "    df_tire_specs = pd.read_excel('tire_specifications.xlsx')\n",
        "    print(\"\\n'tire_specifications.xlsx' loaded successfully!\")\n",
        "    print(\"First 5 rows of df_tire_specs:\")\n",
        "    print(df_tire_specs.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'tire_specifications.xlsx' not found. Please ensure the file is uploaded correctly.\")\n",
        "    df_tire_specs = pd.DataFrame() # Initialize empty DataFrame to avoid errors later\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading tire_specifications.xlsx: {e}\")\n",
        "    df_tire_specs = pd.DataFrame() # Initialize empty DataFrame to avoid errors later"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'historical_loading_data.csv' not found. Please ensure the file is uploaded correctly.\n",
            "Error: 'tire_specifications.xlsx' not found. Please ensure the file is uploaded correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afc80e80"
      },
      "source": [
        "It appears that `df_loading_data` and `df_tire_specs` are still empty. This means the files `historical_loading_data.csv` and `tire_specifications.xlsx` have not been successfully loaded yet.\n",
        "\n",
        "Please ensure you have uploaded these files to the Colab environment using the folder icon on the left panel, and then **re-run the code cells for data loading (the one containing `pd.read_csv` and `pd.read_excel`)** before proceeding. Once the data is loaded, you can re-run the exploration cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8216dc81"
      },
      "source": [
        "It appears that `df_loading_data` and `df_tire_specs` are still empty. This means the files `historical_loading_data.csv` and `tire_specifications.xlsx` have not been successfully loaded yet.\n",
        "\n",
        "Please ensure you have uploaded these files to the Colab environment using the folder icon on the left panel, and then **re-run the code cells for data loading (the one containing `pd.read_csv` and `pd.read_excel`)** before proceeding. Once the data is loaded, you can re-run the exploration cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35904e55"
      },
      "source": [
        "It appears that `df_loading_data` and `df_tire_specs` are still empty. This means the files `historical_loading_data.csv` and `tire_specifications.xlsx` have not been successfully loaded yet.\n",
        "\n",
        "Please ensure you have uploaded these files to the Colab environment using the folder icon on the left panel, and then **re-run the code cells for data loading (the one containing `pd.read_csv` and `pd.read_excel`)** before proceeding. Once the data is loaded, you can re-run the exploration cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9b94ef6"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The primary issue encountered was the consistent failure to load the required data files, `historical_loading_data.csv` and `tire_specifications.xlsx`, from the Colab environment.\n",
        "*   During both the initial attempt and subsequent retries, `FileNotFoundError` exceptions were raised for both files, indicating their absence.\n",
        "*   As a result, the `df_loading_data` and `df_tire_specs` DataFrames remained empty throughout the process, preventing any data exploration or preprocessing.\n",
        "*   The agent provided clear instructions for uploading the necessary files, but these instructions were not followed, leading to the subtask's failure.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The immediate next step is to ensure that the `historical_loading_data.csv` and `tire_specifications.xlsx` files are successfully uploaded to the Colab environment.\n",
        "*   Once the files are uploaded, the data loading and initial exploration steps need to be re-executed to proceed with data preprocessing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1978f251"
      },
      "source": [
        "# Task\n",
        "Define the input schema for user-provided container dimensions, tire specifications, and desired quantities, and implement methods to acquire this direct input for tire loading optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62314132"
      },
      "source": [
        "## Input Data Acquisition and Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Define and implement methods to acquire and preprocess direct user-provided input data (container dimensions, tire specifications, and desired quantities) for subsequent machine learning and optimization tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09c8e352"
      },
      "source": [
        "### Input Schema for User-Provided Data\n",
        "\n",
        "To optimize tire loading, the agent requires the following user-provided inputs:\n",
        "\n",
        "1.  **Container Dimensions:**\n",
        "    *   `container_length` (float): The internal length of the container in a specified unit (e.g., meters).\n",
        "    *   `container_width` (float): The internal width of the container in a specified unit.\n",
        "    *   `container_height` (float): The internal height of the container in a specified unit.\n",
        "\n",
        "2.  **Tire Specifications (for each tire type):**\n",
        "    *   `tire_type` (string): A unique identifier or name for the tire (e.g., 'Truck Tire', 'Car Tire A').\n",
        "    *   `diameter` (float): The overall diameter of the tire in a specified unit.\n",
        "    *   `width` (float): The width of the tire in a specified unit.\n",
        "    *   `quantity` (integer): The number of tires of this specific type to be loaded.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "214133b1"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the input schema is defined, the next step is to implement the interactive input acquisition for container dimensions and tire specifications, along with basic data validation, as specified in instructions 2 and 3 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d7ff67c",
        "outputId": "22caf3e9-7751-442f-de03-ff34b2899e47"
      },
      "source": [
        "def get_positive_float_input(prompt_message):\n",
        "    while True:\n",
        "        try:\n",
        "            value = float(input(prompt_message))\n",
        "            if value <= 0:\n",
        "                print(\"Input must be a positive number. Please try again.\")\n",
        "            else:\n",
        "                return value\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a numerical value.\")\n",
        "\n",
        "def get_positive_int_input(prompt_message):\n",
        "    while True:\n",
        "        try:\n",
        "            value = int(input(prompt_message))\n",
        "            if value <= 0:\n",
        "                print(\"Input must be a positive integer. Please try again.\")\n",
        "            else:\n",
        "                return value\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter an integer value.\")\n",
        "\n",
        "# 1. Acquire Container Dimensions\n",
        "print(\"\\nEnter Container Dimensions (in your chosen unit, e.g., meters):\")\n",
        "container_dimensions = {\n",
        "    'container_length': get_positive_float_input(\"  Enter container length: \"),\n",
        "    'container_width': get_positive_float_input(\"  Enter container width: \"),\n",
        "    'container_height': get_positive_float_input(\"  Enter container height: \")\n",
        "}\n",
        "\n",
        "# 2. Acquire Tire Specifications\n",
        "tire_specifications = []\n",
        "print(\"\\nEnter Tire Specifications (enter 'done' for tire type when finished):\")\n",
        "\n",
        "while True:\n",
        "    tire_type_name = input(\"  Enter tire type (e.g., 'Truck Tire', or 'done' to finish): \").strip()\n",
        "    if tire_type_name.lower() == 'done':\n",
        "        break\n",
        "    elif not tire_type_name:\n",
        "        print(\"Tire type cannot be empty. Please enter a name or 'done'.\")\n",
        "        continue\n",
        "\n",
        "    diameter = get_positive_float_input(f\"    Enter diameter for {tire_type_name}: \")\n",
        "    width = get_positive_float_input(f\"    Enter width for {tire_type_name}: \")\n",
        "    quantity = get_positive_int_input(f\"    Enter quantity for {tire_type_name}: \")\n",
        "\n",
        "    tire_specifications.append({\n",
        "        'tire_type': tire_type_name,\n",
        "        'diameter': diameter,\n",
        "        'width': width,\n",
        "        'quantity': quantity\n",
        "    })\n",
        "\n",
        "print(\"Input acquisition complete.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter Container Dimensions (in your chosen unit, e.g., meters):\n",
            "  Enter container length: 40 feet\n",
            "Invalid input. Please enter a numerical value.\n",
            "  Enter container length: 40\n",
            "  Enter container width: 8\n",
            "  Enter container height: 8.5\n",
            "\n",
            "Enter Tire Specifications (enter 'done' for tire type when finished):\n",
            "  Enter tire type (e.g., 'Truck Tire', or 'done' to finish): Passenger tire\n",
            "    Enter diameter for Passenger tire: 16\n",
            "    Enter width for Passenger tire: 60\n",
            "    Enter quantity for Passenger tire: 100\n",
            "  Enter tire type (e.g., 'Truck Tire', or 'done' to finish): Done\n",
            "Input acquisition complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d55a5e99"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the interactive input acquisition is complete, the next step is to display the collected container dimensions and tire specifications to verify that the data has been acquired and stored correctly, as specified in instruction 4 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6333d8bc",
        "outputId": "13ab123c-4e3c-4ed6-fcff-8b5fa179a6ef"
      },
      "source": [
        "print(\"\\n--- Collected Container Dimensions ---\")\n",
        "for key, value in container_dimensions.items():\n",
        "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "print(\"\\n--- Collected Tire Specifications ---\")\n",
        "if tire_specifications:\n",
        "    for i, tire in enumerate(tire_specifications):\n",
        "        print(f\"Tire Type {i+1}:\")\n",
        "        for key, value in tire.items():\n",
        "            print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
        "else:\n",
        "    print(\"No tire specifications were entered.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Collected Container Dimensions ---\n",
            "Container Length: 40.0\n",
            "Container Width: 8.0\n",
            "Container Height: 8.5\n",
            "\n",
            "--- Collected Tire Specifications ---\n",
            "Tire Type 1:\n",
            "  Tire Type: Passenger tire\n",
            "  Diameter: 16.0\n",
            "  Width: 60.0\n",
            "  Quantity: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a771d81"
      },
      "source": [
        "## Machine Learning Model Development for Fit Prediction\n",
        "\n",
        "### Subtask:\n",
        "Design, train, and evaluate a machine learning model to predict optimal tire size and quantity combinations that will fit, considering different loading methods (normal, fishbone), based on the direct input data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40185e33"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to create a pandas DataFrame by combining the `container_dimensions` with each entry in `tire_specifications`, ensuring each row represents a potential loading scenario. Then, I will engineer new features as specified in the instructions, such as `container_volume`, `tire_volume`, `total_tire_volume_requested`, `volume_ratio`, `max_tires_length`, and `max_tires_width`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "916d49f2",
        "outputId": "7f4f7bf4-b974-4323-e0fe-45db7cd4db4a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a list to store combined data for DataFrame creation\n",
        "combined_data = []\n",
        "\n",
        "# Combine container dimensions with each tire specification\n",
        "for tire in tire_specifications:\n",
        "    row_data = {\n",
        "        'container_length': container_dimensions['container_length'],\n",
        "        'container_width': container_dimensions['container_width'],\n",
        "        'container_height': container_dimensions['container_height'],\n",
        "        'tire_type': tire['tire_type'],\n",
        "        'diameter': tire['diameter'],\n",
        "        'width': tire['width'],\n",
        "        'quantity': tire['quantity']\n",
        "    }\n",
        "    combined_data.append(row_data)\n",
        "\n",
        "# Create the initial DataFrame\n",
        "df_features = pd.DataFrame(combined_data)\n",
        "\n",
        "# Engineer new features\n",
        "# Approximate container volume (assuming rectangular container)\n",
        "df_features['container_volume'] = df_features['container_length'] * df_features['container_width'] * df_features['container_height']\n",
        "\n",
        "# Approximate tire volume (treating tire as a cylinder: pi * (radius)^2 * width)\n",
        "# Radius is diameter / 2\n",
        "df_features['tire_volume'] = np.pi * (df_features['diameter'] / 2)**2 * df_features['width']\n",
        "\n",
        "# Total volume requested for each tire type\n",
        "df_features['total_tire_volume_requested'] = df_features['tire_volume'] * df_features['quantity']\n",
        "\n",
        "# Volume ratio\n",
        "df_features['volume_ratio'] = df_features['total_tire_volume_requested'] / df_features['container_volume']\n",
        "\n",
        "# Max tires that can fit along length and width (simplified, not considering packing density or orientation changes)\n",
        "df_features['max_tires_length'] = df_features['container_length'] / df_features['diameter']\n",
        "df_features['max_tires_width'] = df_features['container_width'] / df_features['width']\n",
        "\n",
        "print(\"Feature engineering complete. Displaying the head of the new DataFrame:\")\n",
        "print(df_features.head())\n",
        "\n",
        "print(\"\\nDisplaying info of the feature DataFrame:\")\n",
        "df_features.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature engineering complete. Displaying the head of the new DataFrame:\n",
            "   container_length  container_width  container_height       tire_type  \\\n",
            "0              40.0              8.0               8.5  Passenger tire   \n",
            "\n",
            "   diameter  width  quantity  container_volume  tire_volume  \\\n",
            "0      16.0   60.0       100            2720.0  12063.71579   \n",
            "\n",
            "   total_tire_volume_requested  volume_ratio  max_tires_length  \\\n",
            "0                 1.206372e+06    443.518963               2.5   \n",
            "\n",
            "   max_tires_width  \n",
            "0         0.133333  \n",
            "\n",
            "Displaying info of the feature DataFrame:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1 entries, 0 to 0\n",
            "Data columns (total 13 columns):\n",
            " #   Column                       Non-Null Count  Dtype  \n",
            "---  ------                       --------------  -----  \n",
            " 0   container_length             1 non-null      float64\n",
            " 1   container_width              1 non-null      float64\n",
            " 2   container_height             1 non-null      float64\n",
            " 3   tire_type                    1 non-null      object \n",
            " 4   diameter                     1 non-null      float64\n",
            " 5   width                        1 non-null      float64\n",
            " 6   quantity                     1 non-null      int64  \n",
            " 7   container_volume             1 non-null      float64\n",
            " 8   tire_volume                  1 non-null      float64\n",
            " 9   total_tire_volume_requested  1 non-null      float64\n",
            " 10  volume_ratio                 1 non-null      float64\n",
            " 11  max_tires_length             1 non-null      float64\n",
            " 12  max_tires_width              1 non-null      float64\n",
            "dtypes: float64(11), int64(1), object(1)\n",
            "memory usage: 236.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a393deb"
      },
      "source": [
        "## Define Target Variable for Fit Prediction\n",
        "\n",
        "### Subtask:\n",
        "Define a target variable, `can_fit`, for the machine learning model based on the engineered features and basic physical constraints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5184fc23"
      },
      "source": [
        "### Note on Target Variable Creation for ML Training\n",
        "\n",
        "It's important to acknowledge that for training a robust machine learning model to predict 'fit' accurately, a significantly larger dataset with labeled outcomes of 'fit' (i.e., historical loading data indicating successful or unsuccessful loading scenarios) would be required. This would allow the model to learn complex patterns and relationships between container dimensions, tire specifications, and actual loading outcomes.\n",
        "\n",
        "For the purpose of this subtask, and given the single input data point, we will define the `can_fit` target variable based on simple, heuristic-driven physical constraints. This will serve as a foundational step for future model development once more comprehensive data is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65fb2eaf"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the context for target variable creation has been set, the next step is to implement the logic to create the `can_fit` column based on the specified physical constraints and volume considerations, as per instructions 2 and 3 of the subtask. This involves checking if the total tire volume fits within the container volume and if individual tire dimensions fit within container dimensions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3e4a3a8",
        "outputId": "bf4ff2e3-7d60-4d1a-e47c-bae8c7e23bb1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create the 'can_fit' column based on heuristic rules\n",
        "df_features['can_fit'] = False # Initialize all as False\n",
        "\n",
        "# Condition 1: Total tire volume requested is less than or equal to container volume\n",
        "volume_condition = (df_features['total_tire_volume_requested'] <= df_features['container_volume'])\n",
        "\n",
        "# Condition 2: Tire dimensions fit within container dimensions\n",
        "# (Simplified check: diameter < length & height, width < container_width)\n",
        "# Assuming tires can be stacked, so diameter is compared with height and length\n",
        "dimension_condition = (\n",
        "    (df_features['diameter'] < df_features['container_length']) &\n",
        "    (df_features['diameter'] < df_features['container_height']) &\n",
        "    (df_features['width'] < df_features['container_width'])\n",
        ")\n",
        "\n",
        "# Combine conditions to determine 'can_fit'\n",
        "df_features.loc[volume_condition & dimension_condition, 'can_fit'] = True\n",
        "\n",
        "print(\"Created 'can_fit' target variable based on heuristic rules.\")\n",
        "print(\"Displaying the 'can_fit' column and relevant features for verification:\")\n",
        "print(df_features[['container_length', 'container_width', 'container_height',\n",
        "                   'diameter', 'width', 'quantity', 'container_volume',\n",
        "                   'total_tire_volume_requested', 'can_fit']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 'can_fit' target variable based on heuristic rules.\n",
            "Displaying the 'can_fit' column and relevant features for verification:\n",
            "   container_length  container_width  container_height  diameter  width  \\\n",
            "0              40.0              8.0               8.5      16.0   60.0   \n",
            "\n",
            "   quantity  container_volume  total_tire_volume_requested  can_fit  \n",
            "0       100            2720.0                 1.206372e+06    False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "534f5123"
      },
      "source": [
        "## Optimization Algorithm Implementation for Space Utilization\n",
        "\n",
        "### Subtask:\n",
        "Formulate the optimization problem for maximizing space utilization within containers/trucks, defining the objective function and constraints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8beb6cd4"
      },
      "source": [
        "### Objective Function and Constraints for Space Utilization\n",
        "\n",
        "Given the input container dimensions and tire specifications, we aim to formulate an optimization problem to maximize space utilization. For this initial stage, we assume a single tire type at a time.\n",
        "\n",
        "#### Objective Function:\n",
        "\n",
        "**Maximize the total number of tires of a given type that can be loaded into the container.**\n",
        "\n",
        "Let $N$ be the number of tires of a specific type to be loaded. The objective is to:\n",
        "\n",
        "$$ \\text{Maximize } N $$\n",
        "\n",
        "#### Constraints:\n",
        "\n",
        "1.  **Volume Constraint (considering packing efficiency):**\n",
        "    The total volume occupied by the tires must not exceed the container's usable volume. We introduce a packing efficiency factor (PEF) to account for the space lost due to irregular packing. A typical PEF for cylindrical objects is around 0.6 to 0.7.\n",
        "\n",
        "    Let $V_{\\text{tire}}$ be the volume of a single tire, $V_{\\text{container}}$ be the volume of the container, and PEF be the packing efficiency factor.\n",
        "\n",
        "    $$ N \\times V_{\\text{tire}} \\le V_{\\text{container}} \\times \\text{PEF} $$\n",
        "\n",
        "2.  **Dimensional Constraints (Length, Width, Height):**\n",
        "    The number of tires that can be placed along each dimension of the container must respect the container's and the tire's dimensions. We consider two main orientations for tires: standing upright (stacked) or lying flat (rolled). For simplicity, we assume tires are primarily oriented to maximize density (e.g., stacked on their widths).\n",
        "\n",
        "    Let $L_c, W_c, H_c$ be the container's length, width, and height, respectively.\n",
        "    Let $D_t, W_t$ be the tire's diameter and width, respectively.\n",
        "\n",
        "    *   **Along Container Length:** The number of tires placed along the length, considering their diameter.\n",
        "        $$ N_L \\le \\left\\lfloor \\frac{L_c}{D_t} \\right\\rfloor $$\n",
        "\n",
        "    *   **Along Container Width:** The number of tires placed along the width, considering their width (if stacked) or diameter (if laid flat).\n",
        "        $$ N_W \\le \\left\\lfloor \\frac{W_c}{W_t} \\right\\rfloor \\quad \\text{ (if stacked)} $$\n",
        "\n",
        "    *   **Along Container Height:** The number of layers of tires that can be stacked, considering their width (if stacked) or diameter (if laid flat).\n",
        "        $$ N_H \\le \\left\\lfloor \\frac{H_c}{W_t} \\right\\rfloor \\quad \\text{ (if stacked)} $$\n",
        "\n",
        "    The total number of tires $N$ must fit within these dimensional limits, considering a 3D arrangement. A simplified constraint could be:\n",
        "\n",
        "    $$ N \\le N_L \\times N_W \\times N_H $$\n",
        "    (This simplified constraint assumes a simple cuboid packing without advanced arrangements).\n",
        "\n",
        "3.  **Quantity Constraint:**\n",
        "    The number of loaded tires cannot exceed the quantity requested by the user for that specific tire type.\n",
        "\n",
        "    Let $Q_{\\text{requested}}$ be the user-specified quantity.\n",
        "\n",
        "    $$ N \\le Q_{\\text{requested}} $$\n",
        "\n",
        "4.  **Non-negativity and Integer Constraint:**\n",
        "    The number of tires must be a non-negative integer.\n",
        "\n",
        "    $$ N \\in \\mathbb{Z}_{\\ge 0} $$\n",
        "\n",
        "#### Assumptions Made:\n",
        "\n",
        "*   **Single Tire Type:** For this initial formulation, we assume the optimization is performed for one tire type at a time. Mixing different tire types will introduce further complexity in packing. (The previous steps' input acquisition allows for multiple tire types, but this formulation simplifies it for a single type).\n",
        "*   **Simplified Packing:** The dimensional constraints assume a basic cuboid-like packing. More advanced packing strategies (e.g., 'fishbone' or hexagonal packing) are not considered at this stage and would yield higher packing efficiency.\n",
        "*   **Fixed Orientation:** We primarily assume tires are stacked upright, with their width dictating the height/depth of a stack and diameter dictating length/width. Alternate orientations are not explicitly modeled in detail yet.\n",
        "*   **Packing Efficiency Factor (PEF):** A generic PEF is used. In reality, PEF can vary based on tire size, container dimensions, and specific packing strategies.\n",
        "*   **No Internal Obstructions:** The container is assumed to be an empty cuboid with no internal obstructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18e5ed44"
      },
      "source": [
        "## Implement Optimization Algorithm\n",
        "\n",
        "### Subtask:\n",
        "Implement an optimization algorithm to determine the maximum number of tires of a single type that can be loaded into the container based on the defined objective function and constraints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5594221"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the features and target variable are defined, the next step is to implement the optimization algorithm as per the instructions. This involves calculating the maximum number of tires that can fit based on volume and dimensional constraints, and then determining the overall optimal quantity to fit.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da79d8fb",
        "outputId": "f7ae5a64-bcde-444f-c1f5-c1a9b206cf08"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Define a packing efficiency factor (PEF)\n",
        "PEF = 0.65 # A typical value for packing efficiency, accounting for space lost during packing\n",
        "\n",
        "# 2. Calculate the maximum number of tires that can fit based on the volume constraint\n",
        "# Ensure tire_volume is not zero to avoid division by zero\n",
        "df_features['max_tires_volume'] = (df_features['container_volume'] * PEF) / df_features['tire_volume'].replace(0, np.nan)\n",
        "\n",
        "# 3. Calculate the maximum number of tires that can fit based on dimensional constraints\n",
        "# Assuming tires are placed upright, where width affects stacking height/depth and diameter affects length/width positioning\n",
        "# Ensure diameter and width are not zero\n",
        "df_features['max_tires_length_dim'] = np.floor(df_features['container_length'] / df_features['diameter'].replace(0, np.nan))\n",
        "df_features['max_tires_width_dim'] = np.floor(df_features['container_width'] / df_features['width'].replace(0, np.nan))\n",
        "df_features['max_tires_height_dim'] = np.floor(df_features['container_height'] / df_features['width'].replace(0, np.nan))\n",
        "\n",
        "# Calculate overall dimensional constraint\n",
        "df_features['max_tires_dimensional'] = df_features['max_tires_length_dim'] * df_features['max_tires_width_dim'] * df_features['max_tires_height_dim']\n",
        "\n",
        "# Fill NaN values that may result from division by zero with a very large number so they don't interfere with np.minimum\n",
        "df_features['max_tires_volume'].fillna(np.inf, inplace=True)\n",
        "df_features['max_tires_dimensional'].fillna(np.inf, inplace=True)\n",
        "\n",
        "# 4. Determine the overall optimal_quantity_fit\n",
        "# Take the minimum of max_tires_volume, max_tires_dimensional, and the user's quantity request\n",
        "df_features['optimal_quantity_fit'] = np.minimum(df_features['quantity'],\n",
        "                                               np.minimum(df_features['max_tires_volume'], df_features['max_tires_dimensional'])).astype(int)\n",
        "\n",
        "print(\"Optimization algorithm applied and 'optimal_quantity_fit' calculated.\")\n",
        "\n",
        "# 6. Print the df_features DataFrame with relevant columns\n",
        "print(\"\\nResults of Optimization:\")\n",
        "print(df_features[['tire_type', 'quantity', 'optimal_quantity_fit', 'can_fit']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization algorithm applied and 'optimal_quantity_fit' calculated.\n",
            "\n",
            "Results of Optimization:\n",
            "        tire_type  quantity  optimal_quantity_fit  can_fit\n",
            "0  Passenger tire       100                     0    False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-31056614.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_features['max_tires_volume'].fillna(np.inf, inplace=True)\n",
            "/tmp/ipython-input-31056614.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_features['max_tires_dimensional'].fillna(np.inf, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04b97b6c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code produced a `FutureWarning` regarding the use of `inplace=True` with chained assignment. To resolve this, I will update the `.fillna()` calls to assign the result back to the DataFrame column, which is the recommended practice for avoiding this warning and ensuring consistency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74856048",
        "outputId": "96bd351a-119c-4388-9b6a-53b507cbebc5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Define a packing efficiency factor (PEF)\n",
        "PEF = 0.65 # A typical value for packing efficiency, accounting for space lost during packing\n",
        "\n",
        "# 2. Calculate the maximum number of tires that can fit based on the volume constraint\n",
        "# Ensure tire_volume is not zero to avoid division by zero\n",
        "df_features['max_tires_volume'] = (df_features['container_volume'] * PEF) / df_features['tire_volume'].replace(0, np.nan)\n",
        "\n",
        "# 3. Calculate the maximum number of tires that can fit based on dimensional constraints\n",
        "# Assuming tires are placed upright, where width affects stacking height/depth and diameter affects length/width positioning\n",
        "# Ensure diameter and width are not zero\n",
        "df_features['max_tires_length_dim'] = np.floor(df_features['container_length'] / df_features['diameter'].replace(0, np.nan))\n",
        "df_features['max_tires_width_dim'] = np.floor(df_features['container_width'] / df_features['width'].replace(0, np.nan))\n",
        "df_features['max_tires_height_dim'] = np.floor(df_features['container_height'] / df_features['width'].replace(0, np.nan))\n",
        "\n",
        "# Calculate overall dimensional constraint\n",
        "df_features['max_tires_dimensional'] = df_features['max_tires_length_dim'] * df_features['max_tires_width_dim'] * df_features['max_tires_height_dim']\n",
        "\n",
        "# Fill NaN values that may result from division by zero with a very large number so they don't interfere with np.minimum\n",
        "df_features['max_tires_volume'] = df_features['max_tires_volume'].fillna(np.inf)\n",
        "df_features['max_tires_dimensional'] = df_features['max_tires_dimensional'].fillna(np.inf)\n",
        "\n",
        "# 4. Determine the overall optimal_quantity_fit\n",
        "# Take the minimum of max_tires_volume, max_tires_dimensional, and the user's quantity request\n",
        "df_features['optimal_quantity_fit'] = np.minimum(df_features['quantity'],\n",
        "                                               np.minimum(df_features['max_tires_volume'], df_features['max_tires_dimensional'])).astype(int)\n",
        "\n",
        "print(\"Optimization algorithm applied and 'optimal_quantity_fit' calculated.\")\n",
        "\n",
        "# 6. Print the df_features DataFrame with relevant columns\n",
        "print(\"\\nResults of Optimization:\")\n",
        "print(df_features[['tire_type', 'quantity', 'optimal_quantity_fit', 'can_fit']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization algorithm applied and 'optimal_quantity_fit' calculated.\n",
            "\n",
            "Results of Optimization:\n",
            "        tire_type  quantity  optimal_quantity_fit  can_fit\n",
            "0  Passenger tire       100                     0    False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "967b1b08"
      },
      "source": [
        "## Create What-If Scenario Simulation Function\n",
        "\n",
        "### Subtask:\n",
        "Create a function to simulate different tire loading scenarios based on the optimization algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f41b8e18"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to define the `simulate_loading_scenario` function, encapsulating the feature engineering and optimization logic. This directly addresses instructions 1, 2, and 3 of the subtask by creating a reusable function that calculates the optimal quantity fit and whether the tires can fit, given specific parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dd6a788",
        "outputId": "d3def057-5a8c-4d75-84a7-5973bddeed53"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def simulate_loading_scenario(\n",
        "    container_length: float,\n",
        "    container_width: float,\n",
        "    container_height: float,\n",
        "    tire_diameter: float,\n",
        "    tire_width: float,\n",
        "    requested_quantity: int,\n",
        "    packing_efficiency_factor: float = 0.65\n",
        ") -> tuple[int, bool]:\n",
        "    \"\"\"\n",
        "    Simulates a tire loading scenario to determine the optimal quantity that can fit\n",
        "    and if the requested quantity can fit based on given container and tire dimensions.\n",
        "\n",
        "    Args:\n",
        "        container_length (float): The internal length of the container.\n",
        "        container_width (float): The internal width of the container.\n",
        "        container_height (float): The internal height of the container.\n",
        "        tire_diameter (float): The diameter of a single tire.\n",
        "        tire_width (float): The width of a single tire.\n",
        "        requested_quantity (int): The number of tires of this type to simulate loading.\n",
        "        packing_efficiency_factor (float): Efficiency factor for volume packing (default 0.65).\n",
        "\n",
        "    Returns:\n",
        "        tuple[int, bool]: A tuple containing the optimal quantity that can fit and\n",
        "                          a boolean indicating if the requested quantity can fit.\n",
        "    \"\"\"\n",
        "\n",
        "    # Feature Engineering\n",
        "    container_volume = container_length * container_width * container_height\n",
        "    tire_volume = np.pi * (tire_diameter / 2)**2 * tire_width\n",
        "    total_tire_volume_requested = tire_volume * requested_quantity\n",
        "\n",
        "    # Optimization Logic\n",
        "    # Volume constraint\n",
        "    if tire_volume == 0:\n",
        "        max_tires_volume = np.inf # Avoid division by zero\n",
        "    else:\n",
        "        max_tires_volume = (container_volume * packing_efficiency_factor) / tire_volume\n",
        "\n",
        "    # Dimensional constraints (assuming tires are placed upright)\n",
        "    if tire_diameter == 0:\n",
        "        max_tires_length_dim = np.inf\n",
        "    else:\n",
        "        max_tires_length_dim = np.floor(container_length / tire_diameter)\n",
        "\n",
        "    if tire_width == 0:\n",
        "        max_tires_width_dim = np.inf\n",
        "        max_tires_height_dim = np.inf\n",
        "    else:\n",
        "        max_tires_width_dim = np.floor(container_width / tire_width)\n",
        "        max_tires_height_dim = np.floor(container_height / tire_width)\n",
        "\n",
        "    max_tires_dimensional = max_tires_length_dim * max_tires_width_dim * max_tires_height_dim\n",
        "\n",
        "    # Overall optimal quantity fit\n",
        "    # Taking the minimum of volume constraint, dimensional constraint, and requested quantity\n",
        "    optimal_quantity_fit = int(np.minimum(requested_quantity,\n",
        "                                        np.minimum(max_tires_volume, max_tires_dimensional)))\n",
        "\n",
        "    # Determine 'can_fit' based on heuristic rules\n",
        "    volume_condition = (total_tire_volume_requested <= container_volume)\n",
        "    dimension_condition = (\n",
        "        (tire_diameter < container_length) and\n",
        "        (tire_diameter < container_height) and\n",
        "        (tire_width < container_width)\n",
        "    )\n",
        "    can_fit = bool(volume_condition and dimension_condition)\n",
        "\n",
        "    return optimal_quantity_fit, can_fit\n",
        "\n",
        "# Demonstrate usage with a 'what-if' scenario (Instructions 4 and 5)\n",
        "print(\"\\n--- Simulating What-If Scenario ---\")\n",
        "\n",
        "# Using the first tire type from the previously collected input for demonstration\n",
        "if tire_specifications:\n",
        "    demo_tire = tire_specifications[0]\n",
        "    # New requested quantity for 'what-if'\n",
        "    what_if_quantity = 10\n",
        "\n",
        "    print(f\"Container Dimensions: Length={container_dimensions['container_length']}, \"\n",
        "          f\"Width={container_dimensions['container_width']}, \"\n",
        "          f\"Height={container_dimensions['container_height']}\")\n",
        "    print(f\"Tire Type: {demo_tire['tire_type']} (Diameter={demo_tire['diameter']}, Width={demo_tire['width']})\")\n",
        "    print(f\"What-if Requested Quantity: {what_if_quantity}\")\n",
        "\n",
        "    # Call the simulation function\n",
        "    simulated_optimal_quantity, simulated_can_fit = simulate_loading_scenario(\n",
        "        container_length=container_dimensions['container_length'],\n",
        "        container_width=container_dimensions['container_width'],\n",
        "        container_height=container_dimensions['container_height'],\n",
        "        tire_diameter=demo_tire['diameter'],\n",
        "        tire_width=demo_tire['width'],\n",
        "        requested_quantity=what_if_quantity,\n",
        "        packing_efficiency_factor=0.65\n",
        "    )\n",
        "\n",
        "    print(f\"Simulated Optimal Quantity Fit: {simulated_optimal_quantity}\")\n",
        "    print(f\"Simulated Can Fit (based on heuristics): {simulated_can_fit}\")\n",
        "else:\n",
        "    print(\"No tire specifications were provided previously to run a what-if scenario.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Simulating What-If Scenario ---\n",
            "Container Dimensions: Length=40.0, Width=8.0, Height=8.5\n",
            "Tire Type: Passenger tire (Diameter=16.0, Width=60.0)\n",
            "What-if Requested Quantity: 10\n",
            "Simulated Optimal Quantity Fit: 0\n",
            "Simulated Can Fit (based on heuristics): False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d84f6ff2"
      },
      "source": [
        "## What-If Scenario Simulation Engine\n",
        "\n",
        "### Subtask:\n",
        "Create a simulation module that allows users to test 'what-if' scenarios with different tire quantities and loading types, based on their direct input, providing recommendations using the trained ML and optimization models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fccf29c"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to define the `run_what_if_simulation` function, which will guide the user to input new or modified container dimensions and tire specifications, use the helper functions for input, call `simulate_loading_scenario`, and then print the results, as specified in instructions 1 through 4 of the subtask. Finally, I will demonstrate its usage by calling it once as per instruction 5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a104cc21",
        "outputId": "dc8b3d70-869d-4296-ba0b-fe37cd2d41e7"
      },
      "source": [
        "def run_what_if_simulation():\n",
        "    \"\"\"\n",
        "    Guides the user to input new container and tire specifications for a 'what-if'\n",
        "    scenario and simulates the loading to provide recommendations.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Running What-If Scenario Simulation ---\\n\")\n",
        "\n",
        "    # Acquire New Container Dimensions\n",
        "    print(\"Enter NEW Container Dimensions (in your chosen unit, e.g., meters):\")\n",
        "    new_container_length = get_positive_float_input(\"  Enter container length: \")\n",
        "    new_container_width = get_positive_float_input(\"  Enter container width: \")\n",
        "    new_container_height = get_positive_float_input(\"  Enter container height: \")\n",
        "\n",
        "    # Acquire New Tire Specifications\n",
        "    print(\"\\nEnter NEW Tire Specifications:\")\n",
        "    new_tire_type_name = input(\"  Enter tire type (e.g., 'Truck Tire'): \").strip()\n",
        "    if not new_tire_type_name:\n",
        "        new_tire_type_name = \"Generic Tire\"\n",
        "        print(f\"  Tire type cannot be empty, using '{new_tire_type_name}'.\")\n",
        "\n",
        "    new_tire_diameter = get_positive_float_input(f\"    Enter diameter for {new_tire_type_name}: \")\n",
        "    new_tire_width = get_positive_float_input(f\"    Enter width for {new_tire_type_name}: \")\n",
        "    new_requested_quantity = get_positive_int_input(f\"    Enter requested quantity for {new_tire_type_name}: \")\n",
        "\n",
        "    print(\"\\nSimulating with the following inputs:\")\n",
        "    print(f\"  Container: L={new_container_length}, W={new_container_width}, H={new_container_height}\")\n",
        "    print(f\"  Tire: Type={new_tire_type_name}, Diameter={new_tire_diameter}, Width={new_tire_width}\")\n",
        "    print(f\"  Requested Quantity: {new_requested_quantity}\")\n",
        "\n",
        "    # Call the simulation function with new parameters\n",
        "    simulated_optimal_quantity, simulated_can_fit = simulate_loading_scenario(\n",
        "        container_length=new_container_length,\n",
        "        container_width=new_container_width,\n",
        "        container_height=new_container_height,\n",
        "        tire_diameter=new_tire_diameter,\n",
        "        tire_width=new_tire_width,\n",
        "        requested_quantity=new_requested_quantity,\n",
        "        packing_efficiency_factor=0.65\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Simulation Results ---\")\n",
        "    print(f\"  Optimal Quantity That Can Fit: {simulated_optimal_quantity}\")\n",
        "    print(f\"  Can the Requested Quantity Fit (Heuristic Check): {simulated_can_fit}\")\n",
        "\n",
        "    if simulated_can_fit and simulated_optimal_quantity >= new_requested_quantity:\n",
        "        print(\"  Recommendation: Yes, the requested quantity of tires can fit.\")\n",
        "    elif simulated_optimal_quantity > 0:\n",
        "        print(f\"  Recommendation: Only {simulated_optimal_quantity} tires can optimally fit out of {new_requested_quantity} requested.\")\n",
        "    else:\n",
        "        print(\"  Recommendation: None of the tires can fit given the dimensions and quantity.\")\n",
        "\n",
        "# Demonstrate how to use the run_what_if_simulation function\n",
        "run_what_if_simulation()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running What-If Scenario Simulation ---\n",
            "\n",
            "Enter NEW Container Dimensions (in your chosen unit, e.g., meters):\n",
            "  Enter container length: 40 ft\n",
            "Invalid input. Please enter a numerical value.\n",
            "  Enter container length: 12.19\n",
            "  Enter container width: 2.44\n",
            "  Enter container height: 2.59\n",
            "\n",
            "Enter NEW Tire Specifications:\n",
            "  Enter tire type (e.g., 'Truck Tire'): Truck Tire\n",
            "    Enter diameter for Truck Tire: 22.5\n",
            "    Enter width for Truck Tire: 295\n",
            "    Enter requested quantity for Truck Tire: 50\n",
            "\n",
            "Simulating with the following inputs:\n",
            "  Container: L=12.19, W=2.44, H=2.59\n",
            "  Tire: Type=Truck Tire, Diameter=22.5, Width=295.0\n",
            "  Requested Quantity: 50\n",
            "\n",
            "--- Simulation Results ---\n",
            "  Optimal Quantity That Can Fit: 0\n",
            "  Can the Requested Quantity Fit (Heuristic Check): False\n",
            "  Recommendation: None of the tires can fit given the dimensions and quantity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76204138"
      },
      "source": [
        "## Real-time Estimation and API Development\n",
        "\n",
        "### Subtask:\n",
        "Develop an API or service that provides real-time estimations of container/truck needs based on user inputs, making the predictive and optimization capabilities accessible and responsive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02f44681"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the necessary components from Flask and initialize the application, as specified in instructions 1 and 2 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74a21ee1",
        "outputId": "4ad5e329-b24f-4e20-840b-ac57c633f6a6"
      },
      "source": [
        "from flask import Flask, request, jsonify\n",
        "\n",
        "# Initialize a Flask application\n",
        "app = Flask(__name__)\n",
        "\n",
        "print(\"Flask app initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask app initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "508c451d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Flask app is initialized, the next step is to define an API endpoint that handles POST requests for tire loading estimation, extracts and validates input parameters, calls the `simulate_loading_scenario` function, and returns a JSON response with the results or an error, as specified in instructions 3-7 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "208d285d",
        "outputId": "5549a5db-382d-4c58-ac4e-724875474b4c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "@app.route('/estimate', methods=['POST'])\n",
        "def estimate_loading():\n",
        "    data = request.get_json()\n",
        "\n",
        "    # Basic input validation\n",
        "    required_params = ['container_length', 'container_width', 'container_height',\n",
        "                       'tire_diameter', 'tire_width', 'requested_quantity']\n",
        "    for param in required_params:\n",
        "        if param not in data:\n",
        "            return jsonify({'error': f'Missing parameter: {param}'}), 400\n",
        "\n",
        "    try:\n",
        "        container_length = float(data['container_length'])\n",
        "        container_width = float(data['container_width'])\n",
        "        container_height = float(data['container_height'])\n",
        "        tire_diameter = float(data['tire_diameter'])\n",
        "        tire_width = float(data['tire_width'])\n",
        "        requested_quantity = int(data['requested_quantity'])\n",
        "\n",
        "        # Validate positive values\n",
        "        if not all(val > 0 for val in [container_length, container_width, container_height, tire_diameter, tire_width, requested_quantity]):\n",
        "            return jsonify({'error': 'All input values must be positive.'}), 400\n",
        "\n",
        "    except ValueError:\n",
        "        return jsonify({'error': 'Invalid data type for one or more parameters. Expected numbers.'}), 400\n",
        "\n",
        "    # Call the simulation function\n",
        "    # The simulate_loading_scenario function is assumed to be defined in a previous cell\n",
        "    optimal_quantity, can_fit = simulate_loading_scenario(\n",
        "        container_length=container_length,\n",
        "        container_width=container_width,\n",
        "        container_height=container_height,\n",
        "        tire_diameter=tire_diameter,\n",
        "        tire_width=tire_width,\n",
        "        requested_quantity=requested_quantity\n",
        "    )\n",
        "\n",
        "    # Construct JSON response\n",
        "    response = {\n",
        "        'optimal_quantity_fit': optimal_quantity,\n",
        "        'can_fit_requested_quantity': can_fit,\n",
        "        'message': 'Estimation successful.'\n",
        "    }\n",
        "    return jsonify(response), 200\n",
        "\n",
        "print(\"API endpoint '/estimate' defined.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API endpoint '/estimate' defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77c47e95"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Flask application and its endpoint have been defined, the next step is to run the Flask application locally within the Colab environment, as specified in instruction 8 of the subtask. This will make the `/estimate` endpoint accessible for testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e677c868",
        "outputId": "caf93a20-d95b-41c0-a6f9-6621c8ebbe40"
      },
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "def run_flask_app():\n",
        "    app.run(port=5000)\n",
        "\n",
        "# Run Flask app in a separate thread to not block the notebook\n",
        "# This is a common pattern in Colab when you want to continue executing cells.\n",
        "flask_thread = threading.Thread(target=run_flask_app)\n",
        "flask_thread.daemon = True # Daemonize thread to stop when main program exits\n",
        "flask_thread.start()\n",
        "\n",
        "print(\"Flask app is running in a background thread on port 5000.\")\n",
        "print(\"You can now send requests to http://127.0.0.1:5000/estimate from other cells.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask app is running in a background thread on port 5000.\n",
            "You can now send requests to http://127.0.0.1:5000/estimate from other cells.\n",
            " * Serving Flask app '__main__'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "887dd84b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Flask app is running, the final step is to demonstrate its accessibility by sending a sample POST request to the `/estimate` endpoint using the `requests` library and printing the JSON response, as specified in instruction 9 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b2cade8",
        "outputId": "f4771f2d-b0ea-4cd9-96d9-2872f1d563e6"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Define the API endpoint URL\n",
        "API_URL = \"http://127.0.0.1:5000/estimate\"\n",
        "\n",
        "# Prepare sample data for the POST request\n",
        "# Using the last 'what-if' scenario inputs for consistency\n",
        "sample_data = {\n",
        "    'container_length': 12.19,\n",
        "    'container_width': 2.44,\n",
        "    'container_height': 2.59,\n",
        "    'tire_diameter': 22.5,\n",
        "    'tire_width': 295.0,\n",
        "    'requested_quantity': 50\n",
        "}\n",
        "\n",
        "print(\"Sending sample POST request to the API...\")\n",
        "\n",
        "try:\n",
        "    # Send the POST request with JSON data\n",
        "    response = requests.post(API_URL, json=sample_data)\n",
        "\n",
        "    # Print the response status code and JSON content\n",
        "    print(f\"Status Code: {response.status_code}\")\n",
        "    print(\"Response JSON:\")\n",
        "    print(json.dumps(response.json(), indent=2))\n",
        "\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(f\"Error: Could not connect to the Flask app at {API_URL}. \")\n",
        "    print(\"Please ensure the Flask app is running (re-run the previous cell if necessary).\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Jan/2026 08:04:54] \"POST /estimate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending sample POST request to the API...\n",
            "Status Code: 200\n",
            "Response JSON:\n",
            "{\n",
            "  \"can_fit_requested_quantity\": false,\n",
            "  \"message\": \"Estimation successful.\",\n",
            "  \"optimal_quantity_fit\": 0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "801cd4b2"
      },
      "source": [
        "**Reasoning**:\n",
        "All instructions for the current subtask, 'Real-time Estimation and API Development', have been successfully executed, including initializing the Flask app, defining the API endpoint, running the app in a background thread, and successfully sending a POST request to it. The API returned the expected simulation results without errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54c3c690"
      },
      "source": [
        "# Task\n",
        "Summarize the capabilities developed so far, detailing how the system processes direct input for container dimensions and tire specifications, predicts tire fit, optimizes space utilization by calculating the maximum possible number of tires, and facilitates 'what-if' scenario simulations, along with the implementation of a real-time estimation API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b0fecc0"
      },
      "source": [
        "## Input Data Acquisition and Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Combine the collected user-provided container dimensions and tire specifications into a structured pandas DataFrame, preparing it for subsequent machine learning and optimization tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb3be4ba"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to create a pandas DataFrame by combining the `container_dimensions` with each entry in `tire_specifications` list into a new DataFrame `df_processed_input` as specified in instructions 1-5, and then display its head to verify the structure as per instruction 6.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37312d38",
        "outputId": "33151c53-ef8c-43e4-9fc6-b8e29ac5647e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Create an empty list called combined_data\n",
        "combined_data = []\n",
        "\n",
        "# 2. Iterate through each tire dictionary in the tire_specifications list\n",
        "# 3. For each tire, create a new dictionary that includes container dimensions and tire specs\n",
        "for tire in tire_specifications:\n",
        "    row_data = {\n",
        "        'container_length': container_dimensions['container_length'],\n",
        "        'container_width': container_dimensions['container_width'],\n",
        "        'container_height': container_dimensions['container_height'],\n",
        "        'tire_type': tire['tire_type'],\n",
        "        'diameter': tire['diameter'],\n",
        "        'width': tire['width'],\n",
        "        'quantity': tire['quantity']\n",
        "    }\n",
        "    # 4. Append this newly created combined dictionary to the combined_data list\n",
        "    combined_data.append(row_data)\n",
        "\n",
        "# 5. Create a pandas DataFrame named df_processed_input from the combined_data list\n",
        "df_processed_input = pd.DataFrame(combined_data)\n",
        "\n",
        "# 6. Display the first few rows of the df_processed_input DataFrame\n",
        "print(\"Created df_processed_input DataFrame:\")\n",
        "print(df_processed_input.head())\n",
        "\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df_processed_input.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created df_processed_input DataFrame:\n",
            "   container_length  container_width  container_height       tire_type  \\\n",
            "0              40.0              8.0               8.5  Passenger tire   \n",
            "\n",
            "   diameter  width  quantity  \n",
            "0      16.0   60.0       100  \n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1 entries, 0 to 0\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   container_length  1 non-null      float64\n",
            " 1   container_width   1 non-null      float64\n",
            " 2   container_height  1 non-null      float64\n",
            " 3   tire_type         1 non-null      object \n",
            " 4   diameter          1 non-null      float64\n",
            " 5   width             1 non-null      float64\n",
            " 6   quantity          1 non-null      int64  \n",
            "dtypes: float64(5), int64(1), object(1)\n",
            "memory usage: 188.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e366abfb"
      },
      "source": [
        "## Machine Learning Model Development for Fit Prediction\n",
        "\n",
        "### Subtask:\n",
        "Design, train, and evaluate a machine learning model to predict optimal tire size and quantity combinations that will fit, considering different loading methods (normal, fishbone), based on the direct input data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89cea697"
      },
      "source": [
        "### Machine Learning Model Development for Fit Prediction: Addressing Limitations\n",
        "\n",
        "**Note**: Given that we currently have only one data point derived from the user's direct input, it is not feasible to train and evaluate a robust machine learning model. Machine learning models require a significant amount of diverse historical data to learn patterns and make accurate predictions.\n",
        "\n",
        "For the purpose of this task and to demonstrate the concept of fit prediction, the `can_fit` column previously created in the `df_features` DataFrame (based on heuristic rules derived from volume and dimensional constraints) will serve as our current 'prediction' for tire fit. This provides a foundational step that would be replaced by a data-driven ML model once sufficient historical loading data becomes available.\n",
        "\n",
        "Below, we display the `df_features` DataFrame, specifically showing the engineered features and the `can_fit` column, which represents our current fit prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8f3b113"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the limitations are acknowledged, the next step is to display the `df_features` DataFrame with the engineered features and the `can_fit` column, as specified in instruction 3 of the subtask, to show the current fit prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "613440c5",
        "outputId": "3942e36c-de36-40a2-c89a-729137ac3907"
      },
      "source": [
        "print(\"Displaying engineered features and the 'can_fit' prediction:\")\n",
        "print(df_features[['container_volume', 'tire_volume', 'total_tire_volume_requested',\n",
        "                   'volume_ratio', 'max_tires_length', 'max_tires_width', 'can_fit']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying engineered features and the 'can_fit' prediction:\n",
            "   container_volume  tire_volume  total_tire_volume_requested  volume_ratio  \\\n",
            "0            2720.0  12063.71579                 1.206372e+06    443.518963   \n",
            "\n",
            "   max_tires_length  max_tires_width  can_fit  \n",
            "0               2.5         0.133333    False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cee1b9b4"
      },
      "source": [
        "## Optimization Algorithm Implementation for Space Utilization\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings from the optimization algorithm, detailing how the maximum possible number of tires and the optimal quantity (considering the requested quantity) are calculated and presented.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02bd3c2b"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to display specific columns from the `df_features` DataFrame. This will show the calculated optimization metrics and the final optimal quantity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61194b43",
        "outputId": "802b9a0f-9b16-437e-f8ab-6352a255b19f"
      },
      "source": [
        "print(\"\\n--- Summary of Optimization Findings ---\")\n",
        "print(\"Displaying key optimization metrics from df_features:\")\n",
        "print(df_features[['tire_type', 'quantity', 'max_tires_volume', 'max_tires_dimensional', 'optimal_quantity_fit', 'can_fit']].round(2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Summary of Optimization Findings ---\n",
            "Displaying key optimization metrics from df_features:\n",
            "        tire_type  quantity  max_tires_volume  max_tires_dimensional  \\\n",
            "0  Passenger tire       100              0.15                    0.0   \n",
            "\n",
            "   optimal_quantity_fit  can_fit  \n",
            "0                     0    False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89cedb36"
      },
      "source": [
        "### Explanation of Optimization Results:\n",
        "\n",
        "*   `tire_type`: Identifies the type of tire for which the optimization was performed (e.g., 'Passenger tire').\n",
        "*   `quantity`: Represents the initial quantity of tires requested by the user to be loaded into the container.\n",
        "*   `max_tires_volume`: This value indicates the theoretical maximum number of tires that could fit into the container based **solely on the container's volume and the tire's volume**, considering a packing efficiency factor (PEF) of 0.65. It provides an upper bound based on volumetric capacity.\n",
        "*   `max_tires_dimensional`: This value represents the maximum number of tires that can fit based on **the container's length, width, and height, and the tire's diameter and width**. It considers how many tires can be placed along each dimension, assuming a basic stacking arrangement. This is often the more restrictive constraint in practical scenarios.\n",
        "*   `optimal_quantity_fit`: This is the final calculated number of tires that can optimally fit. It is derived by taking the **minimum** of the `max_tires_volume`, `max_tires_dimensional`, and the `quantity` requested by the user. This ensures that the suggested quantity respects all physical constraints and does not exceed the user's initial request.\n",
        "*   `can_fit`: This boolean column, derived from heuristic rules, indicates whether the *requested quantity* of tires can fit based on a simplified check of total volume and individual tire dimensions relative to the container.\n",
        "\n",
        "### Interpretation for the current input:\n",
        "\n",
        "For the 'Passenger tire' with a requested `quantity` of 100:\n",
        "\n",
        "*   `max_tires_volume` is 0.15, meaning less than one tire can fit purely based on volume constraints, even with a packing efficiency factor. This suggests the individual tire volume is too large relative to the container, or the container itself is very small.\n",
        "*   `max_tires_dimensional` is 0.0, indicating that dimensionally, zero tires can fit. This is likely due to the tire's diameter (16.0 units) being greater than the container's width (8.0 units) or the tire's width (60.0 units) being greater than the container's width (8.0 units) or height (8.5 units), as seen in the `dimension_condition` evaluation earlier, making it impossible to place even one tire.\n",
        "*   Given these constraints, the `optimal_quantity_fit` is **0**. This means that no tires can be optimally loaded into the container based on the provided dimensions and tire specifications.\n",
        "*   The `can_fit` column is `False`, which aligns with the `optimal_quantity_fit` of 0, confirming that the requested quantity cannot fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9a9a54f"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the capabilities developed so far, focusing on how the system processes direct input, predicts fit, optimizes space utilization (showing maximum possible fit), and handles 'what-if' scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf2ea73a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Direct Input Processing:** The system successfully processes direct user input by combining container dimensions (length, width, height) and tire specifications (type, diameter, width, requested quantity) into a structured pandas DataFrame named `df_processed_input`.\n",
        "*   **Heuristic-based Fit Prediction:** Due to the current limitation of having only one data point, a machine learning model for fit prediction could not be trained. Instead, the system uses a heuristic-based `can_fit` metric (derived from volume and dimensional constraints) as a proxy for prediction. For the provided example, the `can_fit` metric was `False`, indicating that the requested tires cannot fit.\n",
        "*   **Space Utilization Optimization:**\n",
        "    *   The system calculates `max_tires_volume` (maximum tires based on volume and packing efficiency) and `max_tires_dimensional` (maximum tires based on physical dimensions along length, width, and height).\n",
        "    *   For the given input (100 'Passenger tires' in the specified container), `max_tires_volume` was calculated as $0.15$, and `max_tires_dimensional` was $0.0$.\n",
        "    *   The `optimal_quantity_fit` (the final calculated number of tires that can optimally fit) was determined to be $0$. This result was primarily driven by dimensional incompatibility, where the tire's dimensions (diameter $16.0$ units, width $60.0$ units) were too large compared to the container's critical dimensions (e.g., width $8.0$ units, height $8.5$ units), preventing even a single tire from fitting.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current system provides a strong foundation for processing input and performing heuristic-based fit and optimization calculations. To advance, gather more diverse historical data to enable the development and training of a robust machine learning model for more accurate and data-driven fit predictions.\n",
        "*   Further develop the \"what-if\" scenario capabilities by allowing users to easily modify input parameters (container size, tire types/quantities) and immediately visualize the impact on fit prediction and optimal quantity, potentially integrating a user interface for seamless interaction.\n"
      ]
    }
  ]
}